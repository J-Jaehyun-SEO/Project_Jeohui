{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/J-Jaehyun-SEO/Project_Jeohui/blob/main/(1)_Jeohui_data_merge%26preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3meX1VAjYdfz"
      },
      "source": [
        "####(0)Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwzgFMxtl2J9"
      },
      "outputs": [],
      "source": [
        "# 필요한 패키지 설치\n",
        "!pip install --upgrade matplotlib kiwipiepy pandas tqdm nltk gensim scikit-learn flashtext konlpy xlsxwriter\n",
        "!pip install numpy==1.23.5\n",
        "\n",
        "# Nanum 폰트 설치\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm -rf ~/.cache/matplotlib\n",
        "\n",
        "# 라이브러리 임포트\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import itertools\n",
        "import nltk\n",
        "from nltk import collocations\n",
        "from flashtext import KeywordProcessor\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from itertools import combinations\n",
        "from operator import itemgetter\n",
        "import networkx as nx\n",
        "\n",
        "# NLTK 다운로드\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgBgxNVgdbUn"
      },
      "source": [
        "##(1)DATA LOADING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG3XoLGBddnO"
      },
      "source": [
        "###데이터 1 - 조선, 동아 1954-1999"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfrmAXyedUHM"
      },
      "outputs": [],
      "source": [
        "old_news_df = pd.read_csv('/content/저희_조선동아_1954_1999_문장분리.csv')\n",
        "old_news_df=old_news_df.drop(columns=['index','sents','저희_sents','저희_str','저희_문장_index'])\n",
        "old_news_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aT-8ntVAKfXL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from kiwipiepy import Kiwi\n",
        "\n",
        "# Kiwi 형태소 분석기\n",
        "kiwi = Kiwi()\n",
        "\n",
        "# '저희'가 있는 문장 추출 및 저장할 리스트\n",
        "sentences_with_jeohee = []\n",
        "\n",
        "# 문장 구분 기호\n",
        "split_chars = r'[!?。.．。!？]'\n",
        "split_chars_2 = r'[.!?]'\n",
        "\n",
        "# 원본 데이터프레임에 인덱스 추가\n",
        "old_news_df.reset_index(inplace=True)\n",
        "\n",
        "# 'text' 열에서 '저'와 '희' 사이의 스페이스 제거\n",
        "old_news_df['text'] = old_news_df['text'].str.replace(r'저\\s+희', '저희', regex=True)\n",
        "\n",
        "for _, row in old_news_df.iterrows():\n",
        "    # 문장 부호 통일\n",
        "    text = row['text'].replace('。', '.')\n",
        "    text = text.replace('．', '.')\n",
        "    text = text.replace('!', '!')\n",
        "    text = text.replace('？', '?')\n",
        "\n",
        "    # '저희'가 있는 문장 추출\n",
        "    sentences = re.split(split_chars_2, text)  # 통일된 문장 부호 기준으로 분리\n",
        "    for sentence in sentences:\n",
        "        if '저희' in sentence:\n",
        "            if sentence.count('저희') >= 3:\n",
        "                # '저희'가 3개 이상이면 kiwi로 문장 분리\n",
        "                kiwi_sentences = kiwi.split_into_sents(sentence)\n",
        "                for kiwi_sentence in kiwi_sentences:\n",
        "                    if '저희' in kiwi_sentence.text:\n",
        "                        sentences_with_jeohee.append((kiwi_sentence.text.strip(), row['index']))\n",
        "            else:\n",
        "                sentences_with_jeohee.append((sentence.strip(), row['index']))\n",
        "\n",
        "# 새로운 데이터프레임 생성\n",
        "new_df_jeohee = pd.DataFrame(sentences_with_jeohee, columns=['jeohee_sentence', 'original_index'])\n",
        "\n",
        "# '저희' 개수 세는 컬럼 추가\n",
        "new_df_jeohee['jeohee_count'] = new_df_jeohee['jeohee_sentence'].apply(lambda x: x.count('저희'))\n",
        "\n",
        "# 앞, 뒤 문장 컬럼 추가\n",
        "new_df_jeohee['previous_sentence'] = ''\n",
        "new_df_jeohee['next_sentence'] = ''\n",
        "\n",
        "# 앞, 뒤 문장 붙이기\n",
        "for index, row in new_df_jeohee.iterrows():\n",
        "    # 원본 데이터프레임에서 해당 문장 찾기\n",
        "    original_text = old_news_df.loc[row['original_index'], 'text']\n",
        "    jeohee_index = row['original_index']\n",
        "\n",
        "    if jeohee_index is not None:\n",
        "        original_sentences = re.split(split_chars_2, original_text)\n",
        "\n",
        "        # 부분 문자열 포함 여부로 찾기\n",
        "        jeohee_sentence_index = None\n",
        "        for i, s in enumerate(original_sentences):\n",
        "            if row['jeohee_sentence'] in s:\n",
        "                jeohee_sentence_index = i\n",
        "                break\n",
        "\n",
        "        if jeohee_sentence_index is not None:\n",
        "            # 앞 문장 추가\n",
        "            if jeohee_sentence_index > 0:\n",
        "                new_df_jeohee.loc[index, 'previous_sentence'] = original_sentences[jeohee_sentence_index - 1].strip()\n",
        "\n",
        "            # 뒤 문장 추가\n",
        "            if jeohee_sentence_index < len(original_sentences) - 1:\n",
        "                new_df_jeohee.loc[index, 'next_sentence'] = original_sentences[jeohee_sentence_index + 1].strip()\n",
        "\n",
        "# 문장 구분 없는 경우 처리\n",
        "for index, row in new_df_jeohee.iterrows():\n",
        "    if row['previous_sentence'] == '' and row['next_sentence'] == '':\n",
        "        # 문장 구분이 없는 경우 kiwi로 분리\n",
        "        kiwi_sentences = kiwi.split_into_sents(row['jeohee_sentence'])\n",
        "        if len(kiwi_sentences) > 1:\n",
        "            new_df_jeohee.loc[index, 'previous_sentence'] = kiwi_sentences[0].text.strip()\n",
        "            new_df_jeohee.loc[index, 'next_sentence'] = '. '.join([s.text for s in kiwi_sentences[1:]]).strip()\n",
        "\n",
        "# 원본 데이터프레임의 관련 열을 새로운 데이터프레임에 병합\n",
        "new_df_jeohee = new_df_jeohee.merge(old_news_df, left_on='original_index', right_on='index', suffixes=('', '_original'))\n",
        "\n",
        "# 불필요한 인덱스 열 삭제\n",
        "new_df_jeohee.drop(columns=['index', 'original_index'], inplace=True)\n",
        "\n",
        "# 'jeohee_count'가 2 이상인 문장 다시 처리\n",
        "for index, row in new_df_jeohee[ (new_df_jeohee['jeohee_count'] >= 2) | (new_df_jeohee['jeohee_sentence'].str.len() >= 200) ].iterrows():\n",
        "    sentence = row['jeohee_sentence']\n",
        "    kiwi_sentences = kiwi.split_into_sents(sentence)\n",
        "    new_sentences = [s.text for s in kiwi_sentences]\n",
        "\n",
        "    # 문장 재구성\n",
        "    previous_sentence = ''\n",
        "    jeohee_sentence = ''\n",
        "    next_sentence = ''\n",
        "\n",
        "    for i, s in enumerate(new_sentences):\n",
        "        if '저희' in s:\n",
        "            jeohee_sentence = s.strip()\n",
        "            if i > 0:\n",
        "                previous_sentence = new_sentences[i-1].strip()\n",
        "            if i < len(new_sentences) - 1:\n",
        "                next_sentence = new_sentences[i+1].strip()\n",
        "            break\n",
        "\n",
        "    new_df_jeohee.at[index, 'previous_sentence'] = previous_sentence\n",
        "    new_df_jeohee.at[index, 'jeohee_sentence'] = jeohee_sentence\n",
        "    new_df_jeohee.at[index, 'next_sentence'] = next_sentence\n",
        "\n",
        "# 'jeohee_count' 업데이트\n",
        "new_df_jeohee['jeohee_count'] = new_df_jeohee['jeohee_sentence'].apply(lambda x: x.count('저희'))\n",
        "\n",
        "# 결과 확인\n",
        "new_df_jeohee.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXw4aU7I1lQO"
      },
      "outputs": [],
      "source": [
        "jeohee_count_stats = new_df_jeohee['jeohee_count'].value_counts()\n",
        "new_df_jeohee = new_df_jeohee[new_df_jeohee['jeohee_count'] == 1]\n",
        "new_df_jeohee = new_df_jeohee.sort_values('jeohee_count',ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnHX2DVRGfzG"
      },
      "outputs": [],
      "source": [
        "# 문장 부호 치환 함수\n",
        "def replace_punctuation(text):\n",
        "    text = text.replace('。', '.')\n",
        "    text = text.replace('．', '.')\n",
        "    text = text.replace('!', '!')\n",
        "    text = text.replace('？', '?')\n",
        "    text = text.replace('?', '?')\n",
        "\n",
        "    return text\n",
        "\n",
        "# 문장 처리 후 적용할 위치\n",
        "# 원본 데이터프레임의 관련 열을 병합한 후 문장 부호를 치환합니다.\n",
        "new_df_jeohee = new_df_jeohee.merge(old_news_df, left_on='original_index', right_on='index', suffixes=('', '_original'))\n",
        "\n",
        "# replace_punctuation 함수를 'context' 컬럼에 적용\n",
        "new_df_jeohee['context'] = new_df_jeohee['jeohee_sentence'].apply(replace_punctuation)\n",
        "\n",
        "# 불필요한 인덱스 열 삭제\n",
        "new_df_jeohee.drop(columns=['index', 'original_index'], inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNIVAAZGAMmm"
      },
      "source": [
        "저희 개별 문장 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2Yk-N9UK337"
      },
      "outputs": [],
      "source": [
        "# Function to extract sentences containing a specific keyword\n",
        "def extract_sentences(text, word):\n",
        "    sentences = text.split('.')\n",
        "    return '. '.join(sentence.strip() + '.' for sentence in sentences if word.lower() in sentence.lower())\n",
        "\n",
        "# Assuming df is defined earlier and has a column named 'text'\n",
        "\n",
        "# Apply the function to extract sentences containing '저희'\n",
        "new_df['extracted_sentences'] = new_df['context'].apply(lambda x: extract_sentences(x, '저희'))\n",
        "new_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSuY0KYYAPRQ"
      },
      "source": [
        "2 개 나란히 배령된 중복 문장 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJefSmGcN1Wh"
      },
      "outputs": [],
      "source": [
        "new_df['duplicates'] = new_df['extracted_sentences'].duplicated(keep=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lMZ_66031nY"
      },
      "source": [
        "연속해서 저희가 와서 두 문장이 중복되면, 문장을 분할해서 한 문장만 남기기."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cqi5OfQCOTGz"
      },
      "outputs": [],
      "source": [
        "for i in range(1, len(new_df)):\n",
        "    if new_df.loc[i, 'duplicates'] and new_df.loc[i-1, 'extracted_sentences'] == new_df.loc[i, 'extracted_sentences']:\n",
        "        split_sentences = new_df.loc[i, 'extracted_sentences'].split('.')\n",
        "        if split_sentences:\n",
        "            new_df.loc[i-1, 'extracted_sentences'] = split_sentences[0] + '.'\n",
        "            if len(split_sentences) > 1:\n",
        "                new_df.loc[i, 'extracted_sentences'] = '.'.join(split_sentences[1:]) + '.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PX6QL95MlCw"
      },
      "outputs": [],
      "source": [
        "# 중복된 값 찾기\n",
        "duplicates = new_df['extracted_sentences'].duplicated(keep=False)\n",
        "\n",
        "# 중복된 값의 개수 세기\n",
        "num_duplicates = duplicates.sum()\n",
        "\n",
        "print(num_duplicates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I87nW1XRA2Sq"
      },
      "outputs": [],
      "source": [
        "new_df_unique = new_df.drop_duplicates(subset=['extracted_sentences'], keep='first')\n",
        "new_df_unique = new_df_unique[new_df_unique['extracted_sentences'] != '.']\n",
        "new_df_unique\n",
        "new_df=new_df_unique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CNgp40Gdhq3"
      },
      "source": [
        "###데이터 2- 조선, 동아 1990-2024(0525)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJDM6jojdpL9"
      },
      "outputs": [],
      "source": [
        "recent_news_df = pd.read_excel('/content/bigkinds/bigkinds_JOSEON_DONGA.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2Wgc1SNdpGx"
      },
      "outputs": [],
      "source": [
        "#recent_news_df = recent_news_df.drop(columns=['URL'])\n",
        "recent_news_df['year'] = recent_news_df['Published Date'].str[:4]\n",
        "recent_news_df['text'] = recent_news_df['Title'] + ' ' + recent_news_df['Body']\n",
        "recent_news_df=recent_news_df.drop(columns=['URL', 'Category','Published Date','Title','Body'])\n",
        "recent_news_df.rename(columns={'Newspaper': 'publisher'}, inplace=True)\n",
        "recent_news_df['publisher'] = recent_news_df['publisher'].replace({'동아일보': 'donga', '조선일보': 'chosun'})\n",
        "recent_news_df['text'] = recent_news_df['text'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqRCP3A8yfEq"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "special_chars = []\n",
        "for index, row in recent_news_df.iterrows():\n",
        "    text = row['text']\n",
        "    # 특수 문자 추출\n",
        "chars = re.findall(r'[^a-zA-Z0-9ㄱ-ㅎㅏ-ㅣ가-힣\\s\\u2E80-\\u2EFF\\u31C0-\\u31EF\\u3200-\\u32FF\\u3400-\\u4DBF\\u4E00-\\u9FFF\\uF900-\\uFAFF]', text)\n",
        "special_chars.extend(chars)\n",
        "\n",
        "# 중복 제거\n",
        "unique_special_chars = list(set(special_chars))\n",
        "\n",
        "print(unique_special_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEh93sw2Je3H"
      },
      "outputs": [],
      "source": [
        "new_df_jeohee_2 = new_df_jeohee\n",
        "new_df_jeohee_2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####중복 값 검증 및 처리"
      ],
      "metadata": {
        "id": "YjATzXVa2A7U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WYdUnpbbpoI"
      },
      "outputs": [],
      "source": [
        "# 중복된 값 찾기\n",
        "duplicates = new_df_jeohee_2['jeohee_sentence'].duplicated(keep=False)\n",
        "\n",
        "# 중복된 값의 개수 세기\n",
        "num_duplicates = duplicates.sum()\n",
        "\n",
        "print(num_duplicates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfQSOkdc0KW4"
      },
      "source": [
        "중복되는 행들간에 값이 완전히 겹치면 하나만 남기고 제거하고, 총 몇개를 제거했는지 프린트\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2gdb0YozbtN"
      },
      "outputs": [],
      "source": [
        "# 모든 열의 값이 완전히 겹치는 경우 제거\n",
        "before_count = len(new_df_jeohee_2)\n",
        "new_df_jeohee_2.drop_duplicates(subset=new_df_jeohee_2.columns, inplace=True)\n",
        "after_count = len(new_df_jeohee_2)\n",
        "\n",
        "# 제거된 행 개수 출력\n",
        "print(\"제거된 행 개수:\", before_count - after_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CE1JS-oZ0H5_"
      },
      "source": [
        "중에서 jeohee_sentence jeohee_count previous_sentence next_sentence 까지 겹치고 나머지가 다른 경우에는 중복 값을 제거"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3A01aywyz0x-"
      },
      "outputs": [],
      "source": [
        "# 'jeohee_sentence', 'jeohee_count', 'previous_sentence', 'next_sentence' 컬럼 기준으로 중복 제거\n",
        "before_count = len(new_df_jeohee_2)\n",
        "new_df_jeohee_2.drop_duplicates(subset=['jeohee_sentence', 'jeohee_count', 'previous_sentence', 'next_sentence'], inplace=True)\n",
        "after_count = len(new_df_jeohee_2)\n",
        "\n",
        "# 제거된 행 개수 출력\n",
        "print(\"제거된 행 개수:\", before_count - after_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlxJHC21002l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 'jeohee_sentence' 컬럼을 기준으로 그룹화하고, 각 그룹 내에서 다른 컬럼의 차이 확인\n",
        "diff_dfs = []\n",
        "for sentence, group in duplicate_sentences.groupby('jeohee_sentence'):\n",
        "    if len(group) > 1:\n",
        "        diff_df = group[['jeohee_sentence', 'jeohee_count', 'previous_sentence', 'next_sentence']].drop_duplicates()\n",
        "        diff_df['duplicate_group'] = sentence\n",
        "        diff_dfs.append(diff_df)\n",
        "\n",
        "# 차이점을 보여주는 DataFrame 생성\n",
        "result_df = pd.concat(diff_dfs)\n",
        "result_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX_bNGu41zhT"
      },
      "source": [
        "전체 df 에서 위와 같이 중복을 검증해보고, 차이나는 글자수가 50자 이내인 경우, 중복되는 값중 더 글자 수가 작은 값을 제거하고 한 개만 남김"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9NMoEWd1zLJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def remove_duplicates_by_char_count(df):\n",
        "    before_count = len(df)\n",
        "\n",
        "    # 'jeohee_sentence' 컬럼을 기준으로 그룹화\n",
        "    for sentence, group in df.groupby('jeohee_sentence'):\n",
        "        if len(group) > 1:\n",
        "            # 각 그룹 내에서 문자열 길이를 기준으로 정렬\n",
        "            group_sorted = group.copy()\n",
        "            group_sorted['sentence_length'] = group_sorted['jeohee_sentence'].apply(len)\n",
        "            group_sorted = group_sorted.sort_values(by='sentence_length', ascending=True)\n",
        "\n",
        "            # 가장 짧은 문자열과 나머지 문자열들의 길이 차이 계산\n",
        "            shortest_length = group_sorted.iloc[0]['sentence_length']\n",
        "            length_diffs = group_sorted['sentence_length'] - shortest_length\n",
        "\n",
        "            # 길이 차이가 50 이하인 행 제거\n",
        "            rows_to_remove = group_sorted[length_diffs <= 50].index[1:]  # 첫 번째 행 (가장 짧은 문자열)은 제외\n",
        "            df.drop(index=rows_to_remove, inplace=True)\n",
        "\n",
        "    after_count = len(df)\n",
        "    print(\"제거된 행 개수:\", before_count - after_count)\n",
        "    return df\n",
        "\n",
        "\n",
        "# 함수 호출\n",
        "new_df_jeohee_2 = remove_duplicates_by_char_count(new_df_jeohee_2)\n",
        "new_df_jeohee_2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llIAjg3K1cFv"
      },
      "source": [
        "중복행 검증"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncXYgkB5bpoJ"
      },
      "outputs": [],
      "source": [
        "# 중복된 값 찾기\n",
        "duplicates = new_df_jeohee_2['jeohee_sentence'].duplicated(keep=False)\n",
        "\n",
        "# 중복된 값만 필터링\n",
        "duplicate_sentences = new_df_jeohee_2[duplicates]\n",
        "\n",
        "# 중복된 값이 포함된 DataFrame 출력\n",
        "duplicate_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqpY-FA_Y9Rv"
      },
      "source": [
        "###두 데이터 통합해 처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v8jzl0G3LGs"
      },
      "outputs": [],
      "source": [
        "new_df_jeohee_1 = recent_news_df\n",
        "new_df_jeohee_2 = old_news_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKBwjLIi3Mz_"
      },
      "outputs": [],
      "source": [
        "new_df_jeohee_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjbCY4u123eG"
      },
      "outputs": [],
      "source": [
        "new_df_jeohee_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxqFlLuGLebC"
      },
      "outputs": [],
      "source": [
        "# 두 데이터프레임 합치기\n",
        "\n",
        "# 'year' 컬럼을 문자열로 변환\n",
        "new_df_jeohee_1['year'] = new_df_jeohee_1['year'].astype(str)\n",
        "new_df_jeohee_2['year'] = new_df_jeohee_2['year'].astype(str)\n",
        "\n",
        "# 두 데이터프레임을 concat으로 병합\n",
        "merged_df = pd.concat([new_df_jeohee_1, new_df_jeohee_2]).reset_index(drop=True)\n",
        "\n",
        "merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k06Q7ASatbyv"
      },
      "outputs": [],
      "source": [
        "#저회 -> 저희\n",
        "import re\n",
        "\n",
        "# 찾을 패턴 정의\n",
        "pattern = r\"저\\s*회\"\n",
        "\n",
        "# 각 컬럼별로 값 변경\n",
        "for column in ['previous_sentence', 'jeohee_sentence', 'next_sentence']:\n",
        "    new_df_jeohee[column] = new_df_jeohee[column].str.replace(pattern, \"저희\", regex=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##전체 토크나이징 코드"
      ],
      "metadata": {
        "id": "FkL6-vxsyXcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from konlpy.tag import Kkma\n",
        "from kiwipiepy import Kiwi\n",
        "\n",
        "# Kiwi 형태소 분석기\n",
        "kiwi = Kiwi()\n",
        "\n",
        "# 특수기호만 있는 문장 패턴\n",
        "pattern = r\"^[^\\w\\s]+$\"\n",
        "\n",
        "# 원본 데이터프레임에 인덱스 추가\n",
        "old_news_df.reset_index(inplace=True)\n",
        "\n",
        "# 'text' 열에서 '저'와 '희' 사이의 스페이스 제거\n",
        "old_news_df['text'] = old_news_df['text'].str.replace(r'저\\s+희', '저희', regex=True)\n",
        "old_news_df = old_news_df.replace('\\n', '-', regex=True)"
      ],
      "metadata": {
        "id": "Z0cR6t54WIvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 특수 기호들을 처리하는 함수\n",
        "def process_special_characters(text):\n",
        "    # 여는 큰 따옴표로 처리할 특수 기호들\n",
        "    opening_quotation_marks = {'『', '「','‘','“'}\n",
        "    for mark in opening_quotation_marks:\n",
        "        text = text.replace(mark, '\"')\n",
        "\n",
        "    # 닫는 큰 따옴표로 처리할 특수 기호들\n",
        "    closing_quotation_marks = {'』', '」','”',' ’'}\n",
        "    for mark in closing_quotation_marks:\n",
        "        text = text.replace(mark, '\"')\n",
        "    #'…'는 미처리\n",
        "    # '*'으로 처리할 특수 기호들\n",
        "    star_marks = {'○','●', '▼', '◇', '△',  '▲'}\n",
        "    for mark in star_marks:\n",
        "        text = text.replace(mark, '*')\n",
        "\n",
        "    # 제거할 특수 기호들\n",
        "    remove_marks = {'|', '｜','-', '—'}\n",
        "    for mark in remove_marks:\n",
        "        text = text.replace(mark, '')\n",
        "\n",
        "    return text\n",
        "\n",
        "# 데이터프레임의 각 텍스트를 일괄 토크나이징\n",
        "tokenized_texts = []\n",
        "for _, row in old_news_df.iterrows():\n",
        "    text = row['text']\n",
        "\n",
        "    # 특수 기호 처리\n",
        "    text = process_special_characters(text)\n",
        "\n",
        "    # 기존 특수 기호 처리\n",
        "    text = text.replace('。', '.').replace('．', '.').replace('!', '!').replace('？', '?')\n",
        "\n",
        "    # 토크나이징 수행\n",
        "    tokenized_sentences = kiwi.split_into_sents(text)\n",
        "    tokenized_texts.append((tokenized_sentences, row['index']))\n"
      ],
      "metadata": {
        "id": "H8_YNBjNWAUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfwyV-XTsLBD"
      },
      "outputs": [],
      "source": [
        "\n",
        "# '저희'가 포함된 문장과 그 앞뒤 문장을 저장할 리스트\n",
        "sentences_with_jeohee = []\n",
        "\n",
        "# '저희'가 포함된 문장과 그 앞뒤 문장을 추출\n",
        "for tokenized_sentences, idx in tokenized_texts:\n",
        "    sentences = [sentence.text for sentence in tokenized_sentences]\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        if '저희' in sentence:\n",
        "            # 특수기호만 있는 문장은 제외\n",
        "            previous_sentence = sentences[i-1] if i > 0 and not re.match(pattern, sentences[i-1].strip()) else ''\n",
        "            next_sentence = sentences[i+1] if i < len(sentences) - 1 and not re.match(pattern, sentences[i+1].strip()) else ''\n",
        "            sentences_with_jeohee.append((previous_sentence.strip(), sentence.strip(), next_sentence.strip(), idx))\n",
        "\n",
        "# 새로운 데이터프레임 생성\n",
        "new_df_jeohee = pd.DataFrame(sentences_with_jeohee, columns=['previous_sentence', 'jeohee_sentence', 'next_sentence', 'original_index'])\n",
        "\n",
        "# '저희' 개수 세는 컬럼 추가\n",
        "new_df_jeohee['jeohee_count'] = new_df_jeohee['jeohee_sentence'].apply(lambda x: x.count('저희'))\n",
        "\n",
        "# 원본 데이터프레임 병합\n",
        "new_df_jeohee = new_df_jeohee.merge(old_news_df, left_on='original_index', right_on='index', suffixes=('', '_original'))\n",
        "\n",
        "# 불필요한 인덱스 열 삭제\n",
        "new_df_jeohee.drop(columns=['index', 'original_index'], inplace=True)\n",
        "\n",
        "# '저희' 개수 업데이트\n",
        "new_df_jeohee['jeohee_count'] = new_df_jeohee['jeohee_sentence'].apply(lambda x: x.count('저희'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####중복 값 검증 및 처리"
      ],
      "metadata": {
        "id": "-03AtqPC2JML"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64LUhYqR2JMN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def remove_duplicates_by_char_count(df):\n",
        "    before_count = len(df)\n",
        "\n",
        "    # 'jeohee_sentence' 컬럼을 기준으로 그룹화\n",
        "    for sentence, group in df.groupby('jeohee_sentence'):\n",
        "        if len(group) > 1:\n",
        "            # 각 그룹 내에서 문자열 길이를 기준으로 정렬\n",
        "            group_sorted = group.copy()\n",
        "            group_sorted['sentence_length'] = group_sorted['jeohee_sentence'].apply(len)\n",
        "            group_sorted = group_sorted.sort_values(by='sentence_length', ascending=True)\n",
        "\n",
        "            # 가장 짧은 문자열과 나머지 문자열들의 길이 차이 계산\n",
        "            shortest_length = group_sorted.iloc[0]['sentence_length']\n",
        "            length_diffs = group_sorted['sentence_length'] - shortest_length\n",
        "\n",
        "            # 길이 차이가 50 이하인 행 제거\n",
        "            rows_to_remove = group_sorted[length_diffs <= 50].index[1:]  # 첫 번째 행 (가장 짧은 문자열)은 제외\n",
        "            df.drop(index=rows_to_remove, inplace=True)\n",
        "\n",
        "    after_count = len(df)\n",
        "    print(\"제거된 행 개수:\", before_count - after_count)\n",
        "    return df\n",
        "\n",
        "\n",
        "# 함수 호출\n",
        "new_df_jeohee = remove_duplicates_by_char_count(new_df_jeohee)\n",
        "new_df_jeohee\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##학습데이터 추출(레이블링용)"
      ],
      "metadata": {
        "id": "oYbdbN0KzDcj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7IjdEAS46Cu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import xlsxwriter\n",
        "\n",
        "# Output file path\n",
        "df['jeohee_sentence'] = df['jeohee_sentence'].astype(str)\n",
        "output_path = \"//content/학습용데이터/recent_df_0707-추출(sampled_df_2100)백업_1차.xlsx\"\n",
        "\n",
        "# Create a new Excel file and add a worksheet\n",
        "workbook = xlsxwriter.Workbook(output_path)\n",
        "worksheet = workbook.add_worksheet(name='Extracted Sentences')\n",
        "\n",
        "# Define formats\n",
        "red_format = workbook.add_format({'font_color': 'red'})\n",
        "default_format = workbook.add_format({'font_color': 'black'})  # Default formatting\n",
        "\n",
        "# Write headers\n",
        "for col_num, value in enumerate(df.columns):\n",
        "    worksheet.write(0, col_num, value, default_format)\n",
        "\n",
        "# Iterate over rows to apply rich text formatting\n",
        "for idx, row in enumerate(df.itertuples(index=False), start=1):\n",
        "    for col_idx, value in enumerate(row):\n",
        "        if df.columns[col_idx] == 'jeohee_sentence':  # Apply formatting to 'jeohee_sentence' column\n",
        "            if isinstance(value, str) and value:  # Ensure the value is a non-empty string\n",
        "                parts = value.split('저희')\n",
        "                # Prepare the rich text with the appropriate formatting\n",
        "                rich_text = []\n",
        "                if parts[0]:\n",
        "                    rich_text = [default_format, parts[0]]\n",
        "                for part in parts[1:]:\n",
        "                    rich_text.extend([red_format, '저희', default_format, part])\n",
        "                # Write the rich string to the corresponding cell\n",
        "                if rich_text:\n",
        "                    worksheet.write_rich_string(idx, col_idx, *rich_text)\n",
        "                else:\n",
        "                    worksheet.write(idx, col_idx, value, default_format)\n",
        "            else:\n",
        "                worksheet.write(idx, col_idx, value, default_format)\n",
        "        else:\n",
        "            worksheet.write(idx, col_idx, value, default_format)\n",
        "\n",
        "# Close the workbook and save the file\n",
        "workbook.close()\n",
        "print(f\"Filtered data saved to {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 연도별 데이터 비율 계산\n",
        "year_counts = new_df_jeohee['year'].value_counts(normalize=True)\n",
        "\n",
        "# 총 2100건 샘플링\n",
        "sampled_df = pd.DataFrame()\n",
        "for year, proportion in year_counts.items():\n",
        "    num_samples = round(proportion * 2100)\n",
        "    year_samples = new_df_jeohee[new_df_jeohee['year'] == year].sample(n=num_samples, random_state=1)\n",
        "    sampled_df = pd.concat([sampled_df, year_samples])\n",
        "\n",
        "# 인덱스 초기화 및 연도별 정렬\n",
        "sampled_df_2100 = sampled_df.reset_index(drop=True).sort_values(by='year', ascending=True)\n"
      ],
      "metadata": {
        "id": "DO_VOMee3fwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKSZadWv9qfI"
      },
      "source": [
        "###Training 데이터 편집 및 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aQNn5wA9qfO"
      },
      "outputs": [],
      "source": [
        "# '저희' 포함 문장과 그 앞뒤 문장 추출\n",
        "new_rows = []\n",
        "for index, row in recent_news_df.iterrows():\n",
        "    # 'text' 컬럼이 NaN이 아닌 경우에만 처리\n",
        "    if pd.notna(row['text']):\n",
        "        text = str(row['text']).replace('\\n', ' ')  # 'text'를 문자열로 변환\n",
        "\n",
        "        # . , 。 , ! , ? 을 기준으로 문장 분리\n",
        "        sentences = re.split(r'(?<=[\\.!?。． ])', text)\n",
        "\n",
        "        for i, sent in enumerate(sentences):\n",
        "            if '저희' in sent:\n",
        "                new_row = row.to_dict()\n",
        "\n",
        "                context_sentences = []\n",
        "                if i > 0:\n",
        "                    context_sentences.append(sentences[i - 1].strip())\n",
        "                context_sentences.append(sent.strip())\n",
        "                if i < len(sentences) - 1:\n",
        "                    context_sentences.append(sentences[i + 1].strip())\n",
        "\n",
        "                new_row['context'] = ' '.join(context_sentences)\n",
        "                new_rows.append(new_row)\n",
        "\n",
        "new_df = pd.DataFrame(new_rows)\n",
        "\n",
        "# 결과 출력\n",
        "print(new_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8g8IgEiQ_8BG"
      },
      "outputs": [],
      "source": [
        "new_df = new_df.rename(columns={'Newspaper': 'publisher'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFVyBR9VdYml"
      },
      "source": [
        "##분류모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3D8BqZRCC72N"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer, ElectraForSequenceClassification, AdamW\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Paths to dataset files\n",
        "train_path = '/content/data/저희 분류 모델/train_small.csv'\n",
        "test_path = '/content/data/저희 분류 모델/test_small.csv'\n",
        "\n",
        "# Load datasets and print their heads\n",
        "train_dataset = pd.read_csv(train_path)\n",
        "test_dataset = pd.read_csv(test_path)\n",
        "\n",
        "print(\"Train Dataset Head:\")\n",
        "print(train_dataset.head())\n",
        "\n",
        "print(\"\\nTest Dataset Head:\")\n",
        "print(test_dataset.head())\n",
        "\n",
        "# Set up GPU usage\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Dataset Class\n",
        "class ClassifyDataset(Dataset):\n",
        "    def __init__(self, csv_file):\n",
        "        self.dataset = pd.read_csv(csv_file).dropna().drop_duplicates(subset=['sentence'])\n",
        "\n",
        "        # Print the columns for debugging\n",
        "        print(\"Columns in dataset:\", self.dataset.columns)\n",
        "\n",
        "        # Check if '학습용 분류' column exists\n",
        "        if '학습용 분류' not in self.dataset.columns:\n",
        "            raise KeyError(\"The '학습용 분류' column is missing in the dataset\")\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n",
        "        print(self.dataset.describe())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataset.iloc[idx]\n",
        "        text, y = row['sentence'], row['학습용 분류']\n",
        "        inputs = self.tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=256, add_special_tokens=True)\n",
        "        return inputs['input_ids'].squeeze(0), inputs['attention_mask'].squeeze(0), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# Load datasets into DataLoader\n",
        "train_dataset = ClassifyDataset(train_path)\n",
        "test_dataset = ClassifyDataset(test_path)\n",
        "\n",
        "# Initialize Model\n",
        "model = ElectraForSequenceClassification.from_pretrained(\"beomi/KcELECTRA-base\", num_labels=2).to(device)\n",
        "\n",
        "# Fixing the embedding size mismatch\n",
        "model.resize_token_embeddings(50135)\n",
        "\n",
        "# Load pre-trained weights if available\n",
        "model_path = \"/content/data/저희 분류 모델/jh_model.pt\"\n",
        "try:\n",
        "    state_dict = torch.load(model_path)\n",
        "    # Remove unexpected keys and load state dict\n",
        "    model_state_dict = model.state_dict()\n",
        "    for key in state_dict.keys():\n",
        "        if key in model_state_dict and model_state_dict[key].size() == state_dict[key].size():\n",
        "            model_state_dict[key] = state_dict[key]\n",
        "    model.load_state_dict(model_state_dict)\n",
        "except RuntimeError as e:\n",
        "    print(f\"Error loading state_dict: {e}\")\n",
        "    # Handle specific mismatches here if needed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Setup\n",
        "epochs = 5\n",
        "batch_size = 16\n",
        "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Training Loop\n",
        "losses = []\n",
        "accuracies = []\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = total = correct = 0\n",
        "    for input_ids, attention_masks, y_batch in tqdm(train_loader):\n",
        "        input_ids, attention_masks, y_batch = input_ids.to(device), attention_masks.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_masks)\n",
        "        loss = torch.nn.functional.cross_entropy(outputs.logits, y_batch)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        predictions = torch.argmax(outputs.logits, dim=1)\n",
        "        correct += (predictions == y_batch).sum().item()\n",
        "        total += y_batch.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    losses.append(total_loss / len(train_loader))\n",
        "    accuracies.append(accuracy)\n",
        "    print(f\"Epoch {epoch+1}: Loss {losses[-1]:.4f}, Accuracy {accuracies[-1]:.4f}\")\n",
        "\n",
        "# Save Model\n",
        "torch.save(model.state_dict(), \"/content/data/저희 분류 모델/updated_jh_model.pt\")"
      ],
      "metadata": {
        "id": "kiysmNwPVNSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model\n",
        "model.eval()\n",
        "test_correct = test_total = 0\n",
        "for input_ids, attention_masks, y_batch in tqdm(test_loader):\n",
        "    input_ids, attention_masks, y_batch = input_ids.to(device), attention_masks.to(device), y_batch.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_masks)\n",
        "        predictions = torch.argmax(outputs.logits, dim=1)\n",
        "        test_correct += (predictions == y_batch).sum().item()\n",
        "        test_total += y_batch.size(0)\n",
        "\n",
        "# Print Test Accuracy\n",
        "test_accuracy = test_correct / test_total\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Function to classify sentences\n",
        "def classify_sentence(sentence):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n",
        "    model.eval()\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=256, add_special_tokens=True)\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits.squeeze(0).detach().cpu().numpy()\n",
        "    return np.argmax(logits)\n",
        "\n",
        "# Example usage\n",
        "print(classify_sentence(\"여기 예문을 넣으세요\"))"
      ],
      "metadata": {
        "id": "19lXCoVhVPre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24vM9UtNIz2q"
      },
      "outputs": [],
      "source": [
        "print(classify_sentence(\"다 저희 잘못입니다\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMf76nG0I4JP"
      },
      "outputs": [],
      "source": [
        "print(classify_sentence(\"저희 놈들은 죽어 마땅해요!\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1N99zFnTI7Jy"
      },
      "outputs": [],
      "source": [
        "print(classify_sentence(\"저희편인지 우리편인지 구분도 못하는 놈들은 죽어 마땅해요!\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-i7pg7DaGsxp"
      },
      "outputs": [],
      "source": [
        "# GPU 사용\n",
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J263MZEjRMf0"
      },
      "outputs": [],
      "source": [
        "news_df = new_df_jeohee"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpFlk5m2KBEO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, ElectraForSequenceClassification\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 모델과 토크나이저 로드\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n",
        "model = ElectraForSequenceClassification.from_pretrained(\"beomi/KcELECTRA-base\", num_labels=2).to(device)\n",
        "\n",
        "# 가중치 로드 (필요할 경우)\n",
        "model_path = \"/content/data/저희 분류 모델/updated_jh_model.pt\"\n",
        "state_dict = torch.load(model_path, map_location=device)\n",
        "\n",
        "# 모델의 임베딩 레이어 크기 조정\n",
        "model.resize_token_embeddings(50135)\n",
        "\n",
        "# 가중치 로드, strict=False allows ignoring non-matching keys\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "# 문장 추출 및 분류 함수\n",
        "def classify_sentence(sentence):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=256, add_special_tokens=True)\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits.squeeze(0).detach().cpu().numpy()\n",
        "    return np.argmax(logits)\n",
        "\n",
        "def extract_and_classify(df):\n",
        "    results = []\n",
        "    for idx, row in df.iterrows():\n",
        "        for column in ['Title', 'Summary']:\n",
        "            if pd.notna(row[column]):\n",
        "                sentences = row[column].split('. ')\n",
        "                for sentence in sentences:\n",
        "                    if '저희' in sentence:\n",
        "                        label = classify_sentence(sentence)\n",
        "                        results.append({\n",
        "                            \"Sentence\": sentence,\n",
        "                            \"Label\": label,\n",
        "                            \"Column\": column,\n",
        "                            \"Index\": idx,\n",
        "                            \"Newspaper\": row[\"Newspaper\"],\n",
        "                            \"Published Date\": row[\"Published Date\"]\n",
        "                        })\n",
        "    return pd.DataFrame(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xIih-swSEmc"
      },
      "outputs": [],
      "source": [
        "# 결과 추출 및 분류\n",
        "results_df = extract_and_classify(news_df)\n",
        "\n",
        "# 연도 추출 (if results_df is not empty)\n",
        "results_df['Published Date'] = results_df['Published Date'].astype(str)\n",
        "results_df['Published Year'] = results_df['Published Date'].apply(lambda x: x.split('.')[0])\n",
        "\n",
        "# 결과 출력\n",
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTkItg26Spy4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 통계 계산\n",
        "statistics_df = results_df.groupby([\"Newspaper\", \"Published Year\", \"Label\"]).size().reset_index(name='Count')\n",
        "\n",
        "# 통계 출력\n",
        "statistics_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rq91g3bGwfeV"
      },
      "source": [
        "# Dataset 만들어서 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZKIQNjZwdn1"
      },
      "outputs": [],
      "source": [
        "class ClassifyDataset(Dataset):\n",
        "\n",
        "  def __init__(self, csv_file):\n",
        "    # 일부 값중에 NaN이 있음...\n",
        "    self.dataset = pd.read_csv(csv_file).dropna(axis=0)\n",
        "    # 중복제거\n",
        "    self.dataset.drop_duplicates(subset=['sentence'], inplace=True)\n",
        "\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\") #monologg/koelectra-small-v2-discriminator\n",
        "\n",
        "    print(self.dataset.describe())\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    row = self.dataset.iloc[idx, 0:3].values #idx 행과 0,1,2 columns\n",
        "    text = row[0]\n",
        "    y = row[1]\n",
        "\n",
        "    inputs = self.tokenizer(\n",
        "        text,\n",
        "        return_tensors='pt', #return pytorch tensors\n",
        "        truncation=True, #reducing long sequences, 256개의 token만 살리고 뒤는 자름\n",
        "        max_length=256,\n",
        "        pad_to_max_length=True, #padding\n",
        "        add_special_tokens=True #자동으로 문장 앞뒤로 special tocken - padding 부착\n",
        "        )\n",
        "\n",
        "    input_ids = inputs['input_ids'][0] #모델의 입력\n",
        "    attention_mask = inputs['attention_mask'][0] #padding(0이면 패딩 없음)\n",
        "\n",
        "    return input_ids, attention_mask, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESSNkTcXwfUe"
      },
      "outputs": [],
      "source": [
        "train_dataset = ClassifyDataset(train)\n",
        "test_dataset = ClassifyDataset(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJiAJPUDz40W"
      },
      "source": [
        "# Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7-jRPQXz2r5"
      },
      "outputs": [],
      "source": [
        "model = ElectraForSequenceClassification.from_pretrained(\"beomi/KcELECTRA-base\").to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n",
        "sentiment_classifier = TextClassificationPipeline(tokenizer=tokenizer, model=model, device=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edb0aIFaXr4D"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(\"/content/data/저희 분류 모델/jh_model.pt\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmou0LFl0R_X"
      },
      "source": [
        "# Learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NpXwESN0Q4h"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPzxoo4H274J"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-BRNeE226HH"
      },
      "outputs": [],
      "source": [
        "losses = []\n",
        "accuracies = []\n",
        "\n",
        "for i in range(epochs): #epoch 5\n",
        "  total_loss = 0.0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  batches = 0\n",
        "\n",
        "  model.train() #forward\n",
        "\n",
        "  for input_ids_batch, attention_masks_batch, y_batch in tqdm(train_loader): #tqdm 진행상황 확인\n",
        "  # train_loader batch_size = 16 -> iterations에 대해서 batches? (data size / batch size = num of iterations ---> 1 epoch)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    y_batch = y_batch.type(torch.LongTensor)\n",
        "    y_batch = y_batch.to(device)\n",
        "    y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0] #to(device) : gpu에 복사본 저장(pass data to device)\n",
        "    loss = F.cross_entropy(y_pred, y_batch)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step() #update params(weights and biases)\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    _, predicted = torch.max(y_pred, 1) #max로 하는 이유?\n",
        "    correct += (predicted == y_batch).sum()\n",
        "    total += len(y_batch)\n",
        "\n",
        "    batches += 1\n",
        "    if batches % 100 == 0:\n",
        "      print(\"Batch Loss:\", total_loss, \"Accuracy:\", correct.float() / total)\n",
        "\n",
        "  losses.append(total_loss)\n",
        "  accuracies.append(correct.float() / total)\n",
        "  print(\"Train Loss:\", total_loss, \"Accuracy:\", correct.float() / total) #예측한 결과 loss, accuracy (지도학습)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQK4R6n4JgVU"
      },
      "outputs": [],
      "source": [
        "losses, accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkygXZ2n4KW4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Assuming model and datasets are already defined\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "epochs = 5\n",
        "batch_size = 16\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "losses = []\n",
        "accuracies = []\n",
        "\n",
        "for i in range(epochs):\n",
        "    model.train()\n",
        "    total_loss, correct, total, batches = 0.0, 0, 0, 0\n",
        "\n",
        "    for input_ids_batch, attention_masks_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_batch = y_batch.type(torch.LongTensor).to(device)\n",
        "        y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
        "        loss = F.cross_entropy(y_pred, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(y_pred, 1)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "        total += y_batch.size(0)\n",
        "        batches += 1\n",
        "\n",
        "        if batches % 100 == 0:\n",
        "            print(f\"Batch {batches}: Loss: {total_loss / batches:.2f}, Accuracy: {correct / total * 100:.2f}%\")\n",
        "\n",
        "    losses.append(total_loss / batches)\n",
        "    accuracies.append(correct / total)\n",
        "    print(f\"Epoch {i+1}: Train Loss: {losses[-1]:.2f}, Accuracy: {accuracies[-1] * 100:.2f}%\")\n",
        "\n",
        "losses, accuracies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bizON9lm0BNJ"
      },
      "outputs": [],
      "source": [
        "# 모델 저장하기\n",
        "torch.save(model.state_dict(), \"2nd_jh_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTgrQ1ax3j30"
      },
      "outputs": [],
      "source": [
        "from transformers import ElectraForSequenceClassification, ElectraConfig\n",
        "\n",
        "# Ensure you have the correct number of tokens and any other necessary configuration details\n",
        "config = ElectraConfig.from_pretrained('google/electra-small-discriminator', num_labels=2)\n",
        "config.vocab_size = 50135  # This needs to match the saved model's vocab size\n",
        "\n",
        "model = ElectraForSequenceClassification(config)\n",
        "\n",
        "# Load the model\n",
        "model.load_state_dict(torch.load(\"/content/data/저희 분류 모델/2nd_jh_model.pt\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvEB8g7IFbsD"
      },
      "source": [
        "테스트 데이터셋 정확도 확인하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QiALUqm4juf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)  # 모델을 설정된 장치로 이동\n",
        "\n",
        "model.eval()  # 모델을 평가 모드로 설정\n",
        "\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "\n",
        "for input_ids_batch, attention_masks_batch, y_batch in tqdm(test_loader):\n",
        "    # 입력 텐서를 설정된 장치로 이동\n",
        "    input_ids_batch = input_ids_batch.to(device)\n",
        "    attention_masks_batch = attention_masks_batch.to(device)\n",
        "    y_batch = y_batch.to(device).type(torch.LongTensor)  # 장치로 이동 후 타입 변환\n",
        "\n",
        "    # 디버그 메시지 추가\n",
        "    print(\"y_batch device:\", y_batch.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        y_pred = model(input_ids_batch, attention_mask=attention_masks_batch)[0]\n",
        "        _, predicted = torch.max(y_pred, 1)\n",
        "\n",
        "        # 정확한 예측 수 계산\n",
        "        test_correct += (predicted == y_batch).sum().item()\n",
        "        test_total += y_batch.size(0)\n",
        "\n",
        "# 정확도 계산 및 출력\n",
        "accuracy = test_correct / test_total\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ntijcGK0BNK"
      },
      "outputs": [],
      "source": [
        "#문장 하나하나 분류\n",
        "def sentences_predict(sent):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n",
        "    model.eval()\n",
        "    tokenized_sent = tokenizer(\n",
        "            sent,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            add_special_tokens=True,\n",
        "            max_length=256\n",
        "    )\n",
        "    tokenized_sent.to(device)\n",
        "\n",
        "    with torch.no_grad():# 그라디엔트 계산 비활성화\n",
        "        outputs = model(\n",
        "            input_ids=tokenized_sent['input_ids'],\n",
        "            attention_mask=tokenized_sent['attention_mask'],\n",
        "            token_type_ids=tokenized_sent['token_type_ids']\n",
        "            )\n",
        "\n",
        "    logits = outputs[0]\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    result = np.argmax(logits)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnpKXLJX0BNK"
      },
      "source": [
        "# 조선 동아 데이터 분류"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1TVJv8a0BNK"
      },
      "outputs": [],
      "source": [
        "cd_data= pd.read_pickle('./저희_조선동아_1954_1999.pkl')\n",
        "cd_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScZTEl5V0BNL"
      },
      "outputs": [],
      "source": [
        "score = [] # label - score\n",
        "\n",
        "total_len = len(cd_data)\n",
        "\n",
        "for cnt, review in enumerate(cd_data['text']):\n",
        "  pred = sentiment_classifier(review)\n",
        "  score.append(pred)\n",
        "  print(cnt, '개 문장 분류 완료')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKpq5lbF0BNL"
      },
      "outputs": [],
      "source": [
        "len(score) #전체 데이터 개수와 같은지 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VG4Tis3D0BNL"
      },
      "outputs": [],
      "source": [
        "cd_data['predicted'] = 0 # label(예측 결과): 1(긍정) / 0(부정)\n",
        "cd_data['score']=0\n",
        "\n",
        "\n",
        "for i in range(len(score)):\n",
        "    cd_data['predicted'][i] = int(score[i][0].get('label')[-1])\n",
        "    cd_data['score'][i] = float(score[i][0].get('score'))\n",
        "\n",
        "cd_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1703J1_0BNL"
      },
      "outputs": [],
      "source": [
        "cd_data.to_csv('조선동아_저희 모델 분류 결과_10words.csv', index=False)\n",
        "#test_pd.to_excel('저희 모델 분류 결과.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWW6HnMH0BNL"
      },
      "outputs": [],
      "source": [
        "df=cd_data.groupby(['year', 'publisher', 'predicted']).count()\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FE9aPLcA0BNL"
      },
      "outputs": [],
      "source": [
        "df=df[['text']]\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2Lzgh7P0BNL"
      },
      "outputs": [],
      "source": [
        "df.to_excel('./조선동아_저희 모델 분류 결과 추세.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW2yL3uU0BNL"
      },
      "source": [
        "### 시각화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVlMi4cd0BNP"
      },
      "source": [
        "#### 전체"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3GAVljB0BNP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfRr4MS10BNP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "# Mac OS의 경우와 그 외 OS의 경우로 나누어 설정\n",
        "\n",
        "if os.name == 'posix':\n",
        "\n",
        "    plt.rc(\"font\", family=\"AppleGothic\")\n",
        "\n",
        "else :\n",
        "\n",
        "    plt.rc(\"font\", family=\"Malgun Gothic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIQ3XANp0BNP"
      },
      "outputs": [],
      "source": [
        "df=cd_data.groupby(['year', 'predicted']).count()\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyt_QsUt0BNP"
      },
      "outputs": [],
      "source": [
        "df.reset_index(inplace=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc2De6M70BNP"
      },
      "outputs": [],
      "source": [
        "df.set_index(['year'], inplace=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-wbXCzM0BNP"
      },
      "outputs": [],
      "source": [
        "df['겸양의 저희'] =0\n",
        "df['지칭의 저희'] = 0\n",
        "\n",
        "for idx in df.index :\n",
        "    df['겸양의 저희'][idx] = df['text'][df['predicted']==1][idx]\n",
        "\n",
        "    try :\n",
        "        df['지칭의 저희'][idx] =df['text'][df['predicted']==0][idx]\n",
        "    except :\n",
        "    #donga['겸양의 저희'][idx] =0\n",
        "        df['지칭의 저희'][idx] =0\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRB9QUC50BNP"
      },
      "outputs": [],
      "source": [
        "df= df[['겸양의 저희', '지칭의 저희']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BINxzqZ0BNP"
      },
      "outputs": [],
      "source": [
        "df.reset_index(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNTbk3Z90BNP"
      },
      "outputs": [],
      "source": [
        "df= df.drop_duplicates('year')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gldnd0TY0BNQ"
      },
      "outputs": [],
      "source": [
        "df.set_index('year', inplace=True)\n",
        "df.plot()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "3meX1VAjYdfz"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "e815ec34c9d7ca6d5f6dd6120341f05865440853c6373931621ba05a6d8d6b08"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}