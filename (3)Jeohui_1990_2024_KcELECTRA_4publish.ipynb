{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/J-Jaehyun-SEO/Project_Jeohui/blob/main/(3)Jeohui_1990_2024_KcELECTRA_4publish.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##install & setting"
      ],
      "metadata": {
        "id": "Iy6ytMymfrcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "QpJZQPZpdTy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118 --quiet"
      ],
      "metadata": {
        "id": "g30mAVFB5zU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openpyxl adjustText --quiet"
      ],
      "metadata": {
        "id": "szW4P5nwcwvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nanum 폰트 설치\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm -rf ~/.cache/matplotlib"
      ],
      "metadata": {
        "id": "mbH0j9ZLevWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리 임포트\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "from matplotlib import rc\n",
        "\n",
        "# [FONT TEST] 그래프에서 마이너스 폰트 깨지는 문제에 대한 대처\n",
        "plt.rc('font', family='NanumBarunGothic')\n",
        "\n",
        "# plot 한글 깨짐 확인용 -> 한글 깨지면 런타임 재시작 (런타임 메뉴 -> 런타임 다시 시작 후 다시 셀 실행)\n",
        "data = np.random.randint(-100, 100, 50).cumsum()\n",
        "plt.plot(range(50), data, 'r')\n",
        "mpl.rcParams['axes.unicode_minus'] = False\n",
        "plt.title('한자漢字 및 폰트 깨짐 테스트를 위한 그래프')\n",
        "plt.ylabel('font test_한글/漢字')\n",
        "plt.xlabel('!이 그래프의 가로 세로 축 및 범례가 표시된다면 문제 없이 표시되는 것임!')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rHc9RHvbfDe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print('GPU is available!')\n",
        "else:\n",
        "  print('GPU is not available.')"
      ],
      "metadata": {
        "id": "i5qijUxa5aXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgctJp1WY_BX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df = pd.read_pickle('/content/drive/Shareddrives/DHKP/2021-2023.기존연구/03. AI융합연구우리나라_저희(21년_9-12월)/[서재현] 저희/학습용데이터/new_df_0707-특수문자 무시(tokenized_merged)백업_1차.pickle')\n",
        "#df = df[['jeohee_sentence']]  # Keep only the desired column\n",
        "#df = df.rename(columns={'jeohee_sentence': 'sentence'})  #  Rename the column"
      ],
      "metadata": {
        "id": "6NApe47rc2Lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####train"
      ],
      "metadata": {
        "id": "20JvYef1kYj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/drive/Shareddrives/DHKP/2021-2023.기존연구/03. AI융합연구_우리나라_저희(21년_9-12월)/[서재현] 저희/학습용데이터/train.csv')\n",
        "#train = train.rename(columns={'jeohee_sentence': 'sentence'})  # Rename the column\n",
        "train = train.drop('Unnamed: 0', axis=1)\n",
        "train"
      ],
      "metadata": {
        "id": "iT_EjGYH0wXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_length = len(train)\n",
        "filtered_df = train[train['학습용 분류'].isin([0, 1])]\n",
        "removed_count = initial_length - len(filtered_df)\n",
        "print(\"Number of rows removed:\", removed_count)"
      ],
      "metadata": {
        "id": "wh9IdbUBiFNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####test"
      ],
      "metadata": {
        "id": "bG3ZOKaYkclR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'train' is your original DataFrame\n",
        "test = train.sample(n=210, random_state=42)  # Extract 210 random samples\n",
        "train = train.drop(test.index)  # Remove those samples from 'train'\n",
        "\n",
        "# Reset indices for both DataFrames\n",
        "train = train.reset_index(drop=True)\n",
        "test = test.reset_index(drop=True)\n",
        "#test = test.drop('Unnamed: 0', axis=1)\n"
      ],
      "metadata": {
        "id": "jvSo9dXY8VYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "CzWdIIIFdebv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "id": "7ZoZ-QZwiTVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.head())\n",
        "print(test.head())"
      ],
      "metadata": {
        "id": "IKHrZ_Ek0rJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJiAJPUDz40W"
      },
      "source": [
        "# 모델 제작 Create Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model"
      ],
      "metadata": {
        "id": "ndE2ictgoqsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터셋 로드 및 필터링 함수\n"
      ],
      "metadata": {
        "id": "Dbs7Lts4ouxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import ElectraForSequenceClassification, AdamW, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shutil\n",
        "\n",
        "# 디바이스 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 디버깅을 위해 CUDA 런타임 블로킹 설정\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "# 데이터셋 필터링 함수\n",
        "def filter_dataset(dataframe):\n",
        "    return dataframe[dataframe['학습용 분류'].isin([0, 1])]\n",
        "\n",
        "# 데이터 로드 및 필터링\n",
        "train_dataframe = filter_dataset(train)\n",
        "test_dataframe = filter_dataset(test)\n"
      ],
      "metadata": {
        "id": "WkCraB5GoseI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터셋 클래스\n"
      ],
      "metadata": {
        "id": "LC7GulEZovcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassifyDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataset = dataframe.dropna(axis=0).drop_duplicates(subset=['sentence'])\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n",
        "        print(self.dataset.describe())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.dataset.iloc[idx]['sentence']\n",
        "        y = self.dataset.iloc[idx]['학습용 분류']\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            return_tensors='pt',\n",
        "            truncation=True,\n",
        "            max_length=256,\n",
        "            padding='max_length',\n",
        "            add_special_tokens=True\n",
        "        )\n",
        "        input_ids = inputs['input_ids'][0]\n",
        "        attention_mask = inputs['attention_mask'][0]\n",
        "        return input_ids, attention_mask, y\n",
        "\n",
        "# 데이터셋 생성\n",
        "train_dataset = ClassifyDataset(train_dataframe)\n",
        "test_dataset = ClassifyDataset(test_dataframe)\n"
      ],
      "metadata": {
        "id": "9rvmaaF8ow2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 및 토크나이저 설정\n"
      ],
      "metadata": {
        "id": "Gwou2C-gower"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 및 토크나이저 설정\n",
        "model = ElectraForSequenceClassification.from_pretrained(\"beomi/KcELECTRA-base\").to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "epochs = 5\n",
        "batch_size = 16\n",
        "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
        "\n",
        "# 데이터 로더 설정\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "923vGDkSozty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "백업 함수\n"
      ],
      "metadata": {
        "id": "aSRcvAoAo2yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 백업 경로 설정\n",
        "backup_path = '/content/drive/Shareddrives/DHKP/2021-2023.기존연구/03. AI융합연구_우리나라_저희(21년_9-12월)/[서재현] 저희/학습용데이터/'\n",
        "\n",
        "# 백업을 위한 파일 저장 함수\n",
        "def backup_data(epoch, losses, accuracies, model_state, optimizer_state):\n",
        "    data = {\n",
        "        'epoch': epoch,\n",
        "        'losses': losses,\n",
        "        'accuracies': accuracies,\n",
        "        'model_state': model_state,\n",
        "        'optimizer_state': optimizer_state\n",
        "    }\n",
        "    backup_file = f'backup_epoch_{epoch}.pth'\n",
        "    torch.save(data, backup_file)\n",
        "    shutil.move(backup_file, os.path.join(backup_path, backup_file))\n",
        "\n",
        "# 모델 저장 함수\n",
        "def save_model(model, save_path):\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"Model saved to {save_path}\")\n",
        "\n",
        "# 모델 로드 함수\n",
        "def load_model(model_class, load_path, device):\n",
        "    model = model_class.from_pretrained(\"beomi/KcELECTRA-base\")\n",
        "    model.load_state_dict(torch.load(load_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    print(f\"Model loaded from {load_path}\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "w7ejwtubo3w2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_path = os.path.join(backup_path, 'saved_model_240729.pth')\n",
        "load_model()"
      ],
      "metadata": {
        "id": "NYyK4U2EJD0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습 루프"
      ],
      "metadata": {
        "id": "O1kCa9Xlo8or"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "accuracies = []\n",
        "\n",
        "# 학습 루프\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    batches = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for input_ids_batch, attention_masks_batch, y_batch in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids_batch = input_ids_batch.to(device)\n",
        "        attention_masks_batch = attention_masks_batch.to(device)\n",
        "        y_batch = y_batch.to(device, dtype=torch.long)\n",
        "\n",
        "        outputs = model(input_ids_batch, attention_mask=attention_masks_batch)\n",
        "        logits = outputs.logits\n",
        "        loss = F.cross_entropy(logits, y_batch)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "        total += y_batch.size(0)\n",
        "\n",
        "        batches += 1\n",
        "        if batches % 100 == 0:\n",
        "            print(f\"Batch {batches}: Loss {total_loss / batches:.4f}, Accuracy {correct / total:.4f}\")\n",
        "\n",
        "    losses.append(total_loss / batches)\n",
        "    accuracies.append(correct / total)\n",
        "    print(f\"Epoch {epoch + 1}: Train Loss {total_loss / batches:.4f}, Accuracy {correct / total:.4f}\")\n",
        "\n",
        "    # 백업 데이터 저장\n",
        "    backup_data(epoch, losses, accuracies, model.state_dict(), optimizer.state_dict())\n",
        "\n",
        "# 학습 완료 후 모델 저장\n",
        "model_save_path = os.path.join(backup_path, 'saved_model.pth')\n",
        "save_model(model, model_save_path)\n",
        "\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "id": "6-uGWpKZo5oR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "평가 및 예측 저장\n"
      ],
      "metadata": {
        "id": "Q7_GEQ8cpCDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 개별 문장 예측 함수에서 얻은 예측 결과 저장\n",
        "def save_predictions(test_pd, predictions, accuracy):\n",
        "    test_pd['predicted_label'] = predictions\n",
        "    result_file = os.path.join(backup_path, 'predicted_results.csv')\n",
        "    test_pd.to_csv(result_file, index=False)\n",
        "    print(f\"Predictions and accuracy saved to {result_file}\")\n",
        "    with open(os.path.join(backup_path, 'accuracy.txt'), 'w') as f:\n",
        "        f.write(f\"Test Accuracy: {accuracy}\\n\")\n",
        "\n",
        "# 모델 평가 및 예측 결과 저장\n",
        "def evaluate_and_save_predictions(model, test_loader, test_pd, backup_path):\n",
        "    model.eval()\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids_batch, attention_masks_batch, y_batch in tqdm(test_loader):\n",
        "            input_ids_batch = input_ids_batch.to(device)\n",
        "            attention_masks_batch = attention_masks_batch.to(device)\n",
        "            y_batch = y_batch.to(device, dtype=torch.long)\n",
        "\n",
        "            outputs = model(input_ids_batch, attention_mask=attention_masks_batch)\n",
        "            logits = outputs.logits\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            test_correct += (predicted == y_batch).sum().item()\n",
        "            test_total += y_batch.size(0)\n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = test_correct / test_total\n",
        "    print(\"Test Accuracy:\", accuracy)\n",
        "    save_predictions(test_pd, predictions, accuracy)\n",
        "\n",
        "# 모델 평가 및 예측 결과 저장\n",
        "evaluate_and_save_predictions(model, test_loader, test_dataframe, backup_path)\n"
      ],
      "metadata": {
        "id": "8hKbyRxIpBKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "개별 문장 예측\n"
      ],
      "metadata": {
        "id": "4qwagJD4pDk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 개별 문장 예측 함수\n",
        "def sentences_predict(sent):\n",
        "    tokenized_sent = tokenizer(\n",
        "        sent,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        add_special_tokens=True,\n",
        "        max_length=256\n",
        "    )\n",
        "    tokenized_sent = {key: value.to(device) for key, value in tokenized_sent.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=tokenized_sent['input_ids'],\n",
        "            attention_mask=tokenized_sent['attention_mask'],\n",
        "            token_type_ids=tokenized_sent.get('token_type_ids')\n",
        "        )\n",
        "\n",
        "    logits = outputs[0]\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    result = np.argmax(logits)\n",
        "    return result\n",
        "\n",
        "# 개별 문장 예측\n",
        "test_pd = filter_dataset(test_dataframe)  # 데이터 필터링\n",
        "\n",
        "score = []\n",
        "total_len = len(test_pd)\n",
        "\n",
        "# tqdm을 사용하여 간결하게 진행 상황 표시\n",
        "for review in tqdm(test_pd['sentence'], desc=\"Predicting sentences\"):\n",
        "    pred = sentences_predict(review)  # 예측 결과\n",
        "    score.append(pred)\n",
        "\n",
        "# 예측 결과 저장\n",
        "save_predictions(test_pd, score)\n"
      ],
      "metadata": {
        "id": "J-gz3Ek6pDT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3jUDd1n2o7a2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##모델 평가"
      ],
      "metadata": {
        "id": "-yRnWWNqMIIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EoZJPbXdMthg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import ElectraForSequenceClassification, AdamW, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set the CUDA launch blocking (useful for debugging CUDA errors)\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "# Dataset filtering function\n",
        "def filter_dataset(dataframe):\n",
        "    return dataframe[dataframe['학습용 분류'].isin([0, 1])]\n",
        "\n",
        "# Load the dataset\n",
        "def load_data(train_df, test_df):\n",
        "    train_dataframe = filter_dataset(train_df)\n",
        "    test_dataframe = filter_dataset(test_df)\n",
        "    return train_dataframe, test_dataframe\n",
        "\n",
        "# Define Dataset Class\n",
        "class ClassifyDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataset = dataframe.dropna(axis=0).drop_duplicates(subset=['sentence'])\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n",
        "        print(self.dataset.describe())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.dataset.iloc[idx]['sentence']\n",
        "        y = self.dataset.iloc[idx]['학습용 분류']\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            return_tensors='pt',\n",
        "            truncation=True,\n",
        "            max_length=256,\n",
        "            padding='max_length',\n",
        "            add_special_tokens=True\n",
        "        )\n",
        "        input_ids = inputs['input_ids'][0]\n",
        "        attention_mask = inputs['attention_mask'][0]\n",
        "        return input_ids, attention_mask, y\n",
        "\n",
        "# Load the model\n",
        "def load_model(model_class, load_path, device):\n",
        "    model = model_class.from_pretrained(\"beomi/KcELECTRA-base\", num_labels=2)  # Assuming binary classification\n",
        "    model.load_state_dict(torch.load(load_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    print(f\"Model loaded from {load_path}\")\n",
        "    return model\n",
        "\n",
        "# Model prediction function for individual sentences\n",
        "def sentences_predict(sent, model, tokenizer, device):\n",
        "    tokenized_sent = tokenizer(\n",
        "        sent,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        add_special_tokens=True,\n",
        "        max_length=256\n",
        "    )\n",
        "    tokenized_sent = {key: value.to(device) for key, value in tokenized_sent.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=tokenized_sent['input_ids'],\n",
        "            attention_mask=tokenized_sent['attention_mask'],\n",
        "            token_type_ids=tokenized_sent.get('token_type_ids')\n",
        "        )\n",
        "\n",
        "    logits = outputs.logits.detach().cpu().numpy()\n",
        "    result = np.argmax(logits)\n",
        "    return result\n",
        "\n",
        "# Function to evaluate the model using test dataset and calculate metrics\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids_batch, attention_masks_batch, y_batch in tqdm(test_loader):\n",
        "            input_ids_batch = input_ids_batch.to(device)\n",
        "            attention_masks_batch = attention_masks_batch.to(device)\n",
        "            y_batch = y_batch.to(device, dtype=torch.long)\n",
        "\n",
        "            outputs = model(input_ids_batch, attention_mask=attention_masks_batch)\n",
        "            logits = outputs.logits\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "\n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "            true_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='binary')\n",
        "    recall = recall_score(true_labels, predictions, average='binary')\n",
        "    f1 = f1_score(true_labels, predictions, average='binary')\n",
        "\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1 Score: {f1}\")\n",
        "\n",
        "    return accuracy, precision, recall, f1, predictions\n",
        "\n",
        "# Save the predictions and metrics\n",
        "def save_results(test_dataframe, predictions, backup_path):\n",
        "    test_dataframe['predicted_label'] = predictions\n",
        "    result_file = os.path.join(backup_path, 'predicted_results.csv')\n",
        "    test_dataframe.to_csv(result_file, index=False)\n",
        "    print(f\"Results saved to {result_file}\")\n",
        "\n",
        "# Main function to run the process\n",
        "def main(train_df, test_df, backup_path):\n",
        "    # Load and filter datasets\n",
        "    train_dataframe, test_dataframe = load_data(train_df, test_df)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = ClassifyDataset(train_dataframe)\n",
        "    test_dataset = ClassifyDataset(test_dataframe)\n",
        "\n",
        "    # Create data loaders\n",
        "    batch_size = 16\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Load the model\n",
        "    model_save_path = os.path.join(backup_path, 'saved_model_240729.pth')\n",
        "    model = load_model(ElectraForSequenceClassification, model_save_path, device)\n",
        "\n",
        "    # Evaluate the model\n",
        "    accuracy, precision, recall, f1, predictions = evaluate_model(model, test_loader)\n",
        "\n",
        "    # Save the results\n",
        "    save_results(test_dataframe, predictions, backup_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "2QwufZEyMKi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Backup path 설정\n",
        "backup_path = '/content/drive/Shareddrives/DHKP/2021-2023.기존연구/03. AI융합연구_우리나라_저희(21년_9-12월)/[서재현] 저희/학습용데이터/'\n",
        "\n",
        "# train_df와 test_df 파일 경로를 backup_path와 결합\n",
        "train_file_path = os.path.join(backup_path, 'train_240729.csv')  # 학습용 CSV 파일\n",
        "test_file_path = os.path.join(backup_path, 'test_240729.csv')    # 테스트용 CSV 파일\n",
        "\n",
        "# 데이터 로드\n",
        "train_df = pd.read_csv(train_file_path)\n",
        "test_df = pd.read_csv(test_file_path)\n"
      ],
      "metadata": {
        "id": "d01AstV7TKlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# main 함수 실행\n",
        "main(train_df, test_df, backup_path)"
      ],
      "metadata": {
        "id": "OuBkzsqoTN5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Original test dataframe length: {len(test_dataframe)}\")\n",
        "print(f\"Predictions length: {len(predictions)}\")\n"
      ],
      "metadata": {
        "id": "VzneqC4-TrgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('/content/drive/Shareddrives/DHKP/2021-2023.기존연구/03. AI융합연구_우리나라_저희(21년_9-12월)/[서재현] 저희/학습용데이터/test_240729.csv')"
      ],
      "metadata": {
        "id": "MJ8n6pmpPLEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = test.drop('Unnamed: 0', axis=1)\n",
        "test"
      ],
      "metadata": {
        "id": "m9sv3HstQIWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids_batch, attention_masks_batch, y_batch in tqdm(test_loader):\n",
        "            input_ids_batch = input_ids_batch.to(device)\n",
        "            attention_masks_batch = attention_masks_batch.to(device)\n",
        "            y_batch = y_batch.to(device, dtype=torch.long)\n",
        "\n",
        "            # Debug: Check input data\n",
        "            print(f\"input_ids_batch: {input_ids_batch.shape}\")\n",
        "            print(f\"attention_masks_batch: {attention_masks_batch.shape}\")\n",
        "            print(f\"y_batch: {y_batch.shape}\")\n",
        "\n",
        "            outputs = model(input_ids_batch, attention_mask=attention_masks_batch)\n",
        "\n",
        "            logits = outputs.logits\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "\n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "            true_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='binary')\n",
        "    recall = recall_score(true_labels, predictions, average='binary')\n",
        "    f1 = f1_score(true_labels, predictions, average='binary')\n",
        "\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1 Score: {f1}\")\n",
        "\n",
        "    return accuracy, precision, recall, f1, predictions\n"
      ],
      "metadata": {
        "id": "weQRgMmHUW3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model()"
      ],
      "metadata": {
        "id": "bfDnb61PUf_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_class, load_path, device):\n",
        "    model = model_class.from_pretrained(\"beomi/KcELECTRA-base\", num_labels=2)  # Assuming binary classification\n",
        "    model.load_state_dict(torch.load(load_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Debug: Ensure model is loaded\n",
        "    print(f\"Model loaded from {load_path}\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "ceKaKg1nUeCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your paths and run the main function\n",
        "backup_path = '/content/drive/Shareddrives/DHKP/2021-2023.기존연구/03. AI융합연구_우리나라_저희(21년_9-12월)/[서재현] 저희/학습용데이터/'\n",
        "train_df = train # Update with actual path\n",
        "test_df = test_pd    # Update with actual path\n",
        "\n",
        "main(train_df, test_pd, backup_path)"
      ],
      "metadata": {
        "id": "CojiwBT-MzH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "J4zr8-_bNCxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "id": "o8mcs5tNNAVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmou0LFl0R_X"
      },
      "source": [
        "## Learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NpXwESN0Q4h"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPzxoo4H274J"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "batch_size = 16\n",
        "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irvou91IJSiF"
      },
      "outputs": [],
      "source": [
        "# 모델 저장하기\n",
        "torch.save(model.state_dict(), \"jh_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import ElectraForSequenceClassification, AdamW, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 디바이스 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 디버깅을 위해 CUDA 런타임 블로킹 설정\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "# 데이터셋 필터링 함수\n",
        "def filter_dataset(dataframe):\n",
        "    return dataframe[dataframe['학습용 분류'].isin([0, 1])]\n",
        "\n",
        "# 데이터 로드 및 필터링\n",
        "train_dataframe = filter_dataset(train)\n",
        "test_dataframe = filter_dataset(test)\n",
        "\n",
        "class ClassifyDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataset = dataframe.dropna(axis=0).drop_duplicates(subset=['sentence'])\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n",
        "        print(self.dataset.describe())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.dataset.iloc[idx]['sentence']\n",
        "        y = self.dataset.iloc[idx]['학습용 분류']\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            return_tensors='pt',\n",
        "            truncation=True,\n",
        "            max_length=256,\n",
        "            padding='max_length',\n",
        "            add_special_tokens=True\n",
        "        )\n",
        "        input_ids = inputs['input_ids'][0]\n",
        "        attention_mask = inputs['attention_mask'][0]\n",
        "        return input_ids, attention_mask, y\n",
        "\n",
        "# 데이터셋 생성\n",
        "train_dataset = ClassifyDataset(train_dataframe)\n",
        "test_dataset = ClassifyDataset(test_dataframe)\n",
        "\n",
        "# 모델 및 토크나이저 설정\n",
        "model = ElectraForSequenceClassification.from_pretrained(\"beomi/KcELECTRA-base\").to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "epochs = 5\n",
        "batch_size = 16\n",
        "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
        "\n",
        "# 데이터 로더 설정\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "losses = []\n",
        "accuracies = []\n",
        "\n",
        "# 학습 루프\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    batches = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for input_ids_batch, attention_masks_batch, y_batch in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids_batch = input_ids_batch.to(device)\n",
        "        attention_masks_batch = attention_masks_batch.to(device)\n",
        "        y_batch = y_batch.to(device, dtype=torch.long)\n",
        "\n",
        "        outputs = model(input_ids_batch, attention_mask=attention_masks_batch)\n",
        "        logits = outputs.logits\n",
        "        loss = F.cross_entropy(logits, y_batch)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "        total += y_batch.size(0)\n",
        "\n",
        "        batches += 1\n",
        "        if batches % 100 == 0:\n",
        "            print(f\"Batch {batches}: Loss {total_loss / batches:.4f}, Accuracy {correct / total:.4f}\")\n",
        "\n",
        "    losses.append(total_loss / batches)\n",
        "    accuracies.append(correct / total)\n",
        "    print(f\"Epoch {epoch + 1}: Train Loss {total_loss / batches:.4f}, Accuracy {correct / total:.4f}\")\n",
        "\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "id": "UbZ3UWaSgfjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvEB8g7IFbsD"
      },
      "source": [
        "테스트 데이터셋 정확도 확인하기"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가\n",
        "model.eval()\n",
        "\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for input_ids_batch, attention_masks_batch, y_batch in tqdm(test_loader):\n",
        "        input_ids_batch = input_ids_batch.to(device)\n",
        "        attention_masks_batch = attention_masks_batch.to(device)\n",
        "        y_batch = y_batch.to(device, dtype=torch.long)\n",
        "\n",
        "        outputs = model(input_ids_batch, attention_mask=attention_masks_batch)\n",
        "        logits = outputs.logits\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "        test_correct += (predicted == y_batch).sum().item()\n",
        "        test_total += y_batch.size(0)\n",
        "\n",
        "print(\"Test Accuracy:\", test_correct / test_total)"
      ],
      "metadata": {
        "id": "-Df5GtOWlu8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backup_path = '/content/drive/Shareddrives/DHKP/2021-2023.기존연구/03. AI융합연구_우리나라_저희(21년_9-12월)/[서재현] 저희/학습용데이터'"
      ],
      "metadata": {
        "id": "BUjfHnNTmjRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 개별 문장 예측 함수에서 얻은 예측 결과 저장\n",
        "def save_predictions(test_pd, predictions):\n",
        "    test_pd['predicted_label'] = predictions\n",
        "    result_file = os.path.join(backup_path, 'predicted_results.csv')\n",
        "    test_pd.to_csv(result_file, index=False)"
      ],
      "metadata": {
        "id": "OIJTW0lHmLSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "predictions = []\n",
        "\n",
        "# 개별 문장 예측\n",
        "test_pd = filter_dataset(test_dataframe)  # 데이터 필터링\n",
        "\n",
        "score = []\n",
        "total_len = len(test_pd)\n",
        "\n",
        "# tqdm을 사용하여 간결하게 진행 상황 표시\n",
        "for review in tqdm(test_pd['sentence'], desc=\"Predicting sentences\"):\n",
        "    pred = sentences_predict(review)  # 예측 결과\n",
        "    score.append(pred)\n",
        "\n",
        "# 예측 결과 저장\n",
        "save_predictions(test_pd, score)\n",
        "\n",
        "# 예측 결과 출력\n",
        "test_pd['predicted_label'] = score\n",
        "print(test_pd[['sentence', 'predicted_label']])"
      ],
      "metadata": {
        "id": "EhcKeU92mQGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gjpeqzFJSiG"
      },
      "outputs": [],
      "source": [
        "test_pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDMwwta5JSiG"
      },
      "outputs": [],
      "source": [
        "test_pd['score'] = 0\n",
        "test_pd['predicted'] =0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrEEtjwaJSiG"
      },
      "outputs": [],
      "source": [
        "len(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qw7aWoScJSiG"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# 개별 문장 예측 함수\n",
        "def sentences_predict(sent):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n",
        "    model.eval()\n",
        "    tokenized_sent = tokenizer(\n",
        "        sent,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        add_special_tokens=True,\n",
        "        max_length=256\n",
        "    )\n",
        "    tokenized_sent = {key: value.to(device) for key, value in tokenized_sent.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=tokenized_sent['input_ids'],\n",
        "            attention_mask=tokenized_sent['attention_mask'],\n",
        "            token_type_ids=tokenized_sent.get('token_type_ids')\n",
        "        )\n",
        "\n",
        "    logits = outputs[0]\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    result = np.argmax(logits)\n",
        "    return result\n",
        "\n",
        "# 개별 문장 예측\n",
        "test_pd = filter_dataset(test_dataframe)  # 데이터 필터링\n",
        "\n",
        "score = []\n",
        "total_len = len(test_pd)\n",
        "\n",
        "# tqdm을 사용하여 간결하게 진행 상황 표시\n",
        "for review in tqdm(test_pd['sentence'], desc=\"Predicting sentences\"):\n",
        "    pred = sentences_predict(review)  # 예측 결과\n",
        "    score.append(pred)\n",
        "\n",
        "# 예측 결과와 데이터프레임 길이 확인 및 처리\n",
        "if len(score) != len(test_pd):\n",
        "    raise ValueError(f\"Length of predictions ({len(score)}) does not match length of index ({len(test_pd)})\")\n",
        "\n",
        "# 예측 결과 저장\n",
        "def save_predictions(test_pd, predictions):\n",
        "    test_pd['predicted_label'] = predictions\n",
        "    result_file = os.path.join(backup_path, 'predicted_results.csv')\n",
        "    test_pd.to_csv(result_file, index=False)\n",
        "\n",
        "save_predictions(test_pd, score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehPeKvcOJSiG"
      },
      "outputs": [],
      "source": [
        "test_pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQuI6p-uJSiG"
      },
      "outputs": [],
      "source": [
        "!pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfMtv0DMJSiG"
      },
      "outputs": [],
      "source": [
        "#test_pd.to_csv('저희 모델 분류 결과_withscore.csv', index=False)\n",
        "test_pd.to_excel('저희 모델 분류 결과_withscore.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDx-_NUuJSiG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# 모델 저장 경로\n",
        "model_save_path = '/content/drive/Shareddrives/DHKP/2021-2023.기존연구/03. AI융합연구_우리나라_저희(21년_9-12월)/[서재현] 저희/학습용데이터/saved_model.pth'\n",
        "\n",
        "# 모델 저장 함수\n",
        "def save_model(model, save_path):\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"Model saved to {save_path}\")\n",
        "\n",
        "# 학습이 완료된 후 모델 저장\n",
        "save_model(model, model_save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2-vReM2ViFOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-i7pg7DaGsxp"
      },
      "outputs": [],
      "source": [
        "# GPU 사용\n",
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#전체 데이터 분류"
      ],
      "metadata": {
        "id": "znxKj9OMpzCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import ElectraForSequenceClassification, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 디바이스 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 백업 경로 설정\n",
        "backup_path = '/content/drive/Shareddrives/DHKP/2021-2023.기존연구/03. AI융합연구_우리나라_저희(21년_9-12월)/[서재현] 저희/학습용데이터/'\n",
        "model_save_path = os.path.join(backup_path, 'saved_model_240729.pth')"
      ],
      "metadata": {
        "id": "bfRKhB6ap00N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 및 토크나이저 설정\n",
        "model = ElectraForSequenceClassification.from_pretrained(\"beomi/KcELECTRA-base\").to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n",
        "\n",
        "# 모델 로드 함수\n",
        "def load_model(model_class, load_path, device):\n",
        "    model = model_class.from_pretrained(\"beomi/KcELECTRA-base\")\n",
        "    model.load_state_dict(torch.load(load_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    print(f\"Model loaded from {load_path}\")\n",
        "    return model\n",
        "\n",
        "# 모델 로드\n",
        "model = load_model(ElectraForSequenceClassification, model_save_path, device)"
      ],
      "metadata": {
        "id": "x37Vi0XAp3ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 클래스 정의\n",
        "class ClassifyDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataset = dataframe\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.dataset.iloc[idx]['sentence']\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            return_tensors='pt',\n",
        "            truncation=True,\n",
        "            max_length=256,\n",
        "            padding='max_length',\n",
        "            add_special_tokens=True\n",
        "        )\n",
        "        input_ids = inputs['input_ids'][0]\n",
        "        attention_mask = inputs['attention_mask'][0]\n",
        "        return input_ids, attention_mask"
      ],
      "metadata": {
        "id": "HPmEiaA1p47N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####test - 문장 분류 잘함."
      ],
      "metadata": {
        "id": "XvxnFt3Lm8xK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T41xMPVhmXbd"
      },
      "outputs": [],
      "source": [
        "#문장 하나하나 분류\n",
        "def sentences_predict(sent):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n",
        "    model.eval()\n",
        "    tokenized_sent = tokenizer(\n",
        "            sent,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            add_special_tokens=True,\n",
        "            max_length=256\n",
        "    )\n",
        "    tokenized_sent.to(device)\n",
        "\n",
        "    with torch.no_grad():# 그라디엔트 계산 비활성화\n",
        "        outputs = model(\n",
        "            input_ids=tokenized_sent['input_ids'],\n",
        "            attention_mask=tokenized_sent['attention_mask'],\n",
        "            token_type_ids=tokenized_sent['token_type_ids']\n",
        "            )\n",
        "\n",
        "    logits = outputs[0]\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    result = np.argmax(logits)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_predict('저희 쪽에서도 다 생각이 있겠지?')"
      ],
      "metadata": {
        "id": "yRDqviTWmo6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgKKM1k7mXbe"
      },
      "outputs": [],
      "source": [
        "sentences_predict('도대체 어쩌란 말입니까... 저희 잘못인 것은 저희도 알고 있습니다.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u84Z6FTgmXbe"
      },
      "outputs": [],
      "source": [
        "sentences_predict('도대체 어쩌란 말이냐! 저희로 썩 꺼져라')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzCnwcDYmXbe"
      },
      "outputs": [],
      "source": [
        "sentences_predict('도대체 어쩌란 말이냐! \"저희들이 알아서 하겠나이다\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3Ez9FXsmXbe"
      },
      "outputs": [],
      "source": [
        "sentences_predict('넌 너가 무엇을 분류하는지 알고 있느냐? 그건 저희들이 알아서 무얼하려고 물어보는지..')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###분류를 위한 전체 데이터 로드"
      ],
      "metadata": {
        "id": "kAMD5i4BwJEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "third_data = pd.read_pickle('/content/drive/Shareddrives/DHKP/2021-2023.기존연구/03. AI융합연구_우리나라_저희(21년_9-12월)/[서재현] 저희/학습용데이터/new_df_0707-처리완(특수기호_tokenized_merged)백업_2차.pickle')\n",
        "#third_data = third_data[['jeohee_sentence']]  # Keep only the desired column\n",
        "third_data = third_data.rename(columns={'jeohee_sentence': 'sentence'})  #  Rename the column\n",
        "#df['학습용 분류'] = 0"
      ],
      "metadata": {
        "id": "i069-Tkwp9Bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "제3의 데이터셋 생성"
      ],
      "metadata": {
        "id": "Hvi3PtIJwLE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 제3의 데이터셋 생성\n",
        "third_dataset = ClassifyDataset(third_data)\n",
        "third_loader = DataLoader(third_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "b8HBElTqp6T7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 개별 문장 예측 함수\n",
        "def sentences_predict(input_ids, attention_mask):\n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    predicted_labels = torch.argmax(probabilities, dim=1)\n",
        "    return predicted_labels.cpu().numpy(), probabilities.cpu().numpy(), logits.cpu().numpy()\n",
        "\n",
        "predictions = []\n",
        "probabilities_list = []\n",
        "logits_list = []\n",
        "\n",
        "# tqdm을 사용하여 간결하게 진행 상황 표시\n",
        "for input_ids_batch, attention_masks_batch in tqdm(third_loader, desc=\"Predicting third data\"):\n",
        "    preds, probs, logits = sentences_predict(input_ids_batch, attention_masks_batch)\n",
        "    predictions.extend(preds)\n",
        "    probabilities_list.extend(probs)\n",
        "    logits_list.extend(logits)\n",
        "\n",
        "# 예측 결과를 원본 데이터프레임에 추가\n",
        "if len(predictions) == len(third_data):\n",
        "    third_data['predicted_label'] = predictions\n",
        "    if probabilities_list and logits_list:  # 리스트가 비어있지 않은지 확인\n",
        "        for i in range(probabilities_list[0].shape[0]):\n",
        "            third_data[f'prob_class_{i}'] = [prob[i] for prob in probabilities_list]\n",
        "        for i in range(logits_list[0].shape[0]):\n",
        "            third_data[f'logit_class_{i}'] = [logit[i] for logit in logits_list]\n",
        "else:\n",
        "    print(f\"Warning: Length of predictions ({len(predictions)}) does not match length of data ({len(third_data)})\")\n",
        "\n",
        "# 예측 결과 저장\n",
        "third_data.to_csv(os.path.join(backup_path, 'v2_040729_third_data_predictions.csv'), index=False)\n",
        "print(f\"Predictions saved to {os.path.join(backup_path, 'v2_040729_third_data_predictions.csv')}\")"
      ],
      "metadata": {
        "id": "GVToAFKMraAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터셋의 총 샘플 수가 31,967개이고, 배치 크기가 16인 경우: DataLoader는 31,967개의 샘플을 1,998개의 배치로 나눕니다. 각 배치는 16개의 샘플을 포함하며, 마지막 배치는 15개의 샘플을 포함합니다.1,998개의 배치에 대한 진행 상황을 표시"
      ],
      "metadata": {
        "id": "RdcfOvj2yCKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#data analysis"
      ],
      "metadata": {
        "id": "OAcQGblVyvOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "v3_jeohui_040730_third_data_predictions.pickle"
      ],
      "metadata": {
        "id": "hwk9c_rI0auN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import ElectraForSequenceClassification, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Bao8r5_dfaLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_df = pd.read_pickle('/content/drive/Shareddrives/DHKP/2021-2023.기존연구/03. AI융합연구_우리나라_저희(21년_9-12월)/[서재현] 저희/학습용데이터/v3_jeohui_040730_third_data_predictions.pickle')"
      ],
      "metadata": {
        "id": "RnEmlGRjwNkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_df"
      ],
      "metadata": {
        "id": "EqqRmdbZ3dDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##기술통계량"
      ],
      "metadata": {
        "id": "H-j1llch-Mdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the year into decades (including up to 2024)\n",
        "predictions_df['decade'] = pd.cut(predictions_df['year'], bins=[1950, 1960, 1970, 1980, 1990, 2000, 2010, 2020, 2025],\n",
        "                      labels=[\"50s\", \"60s\", \"70s\", \"80s\", \"90s\", \"2000s\", \"2010s\", \"2020s\"])\n",
        "\n",
        "# Group by decade and predicted_label to calculate frequency\n",
        "grouped = predictions_df.groupby(['decade', 'predicted_label']).size().unstack(fill_value=0)\n",
        "\n",
        "# Calculate total for each decade\n",
        "grouped['Total'] = grouped.sum(axis=1)\n",
        "\n",
        "# Calculate percentages for '겸양의 저희' (0) and '저편의 저희' (1)\n",
        "grouped['% 저편'] = (grouped.get(0, 0) / grouped['Total'] * 100).round(2)\n",
        "grouped['% 겸양'] = (grouped.get(1, 0) / grouped['Total'] * 100).round(2)\n",
        "\n",
        "# Add a grand total row for the sum of all decades\n",
        "grand_total = pd.DataFrame(grouped.sum()).transpose()\n",
        "grand_total.index = ['총합']\n",
        "\n",
        "# Concatenate the grand total row to the grouped data\n",
        "grouped = pd.concat([grouped, grand_total])\n",
        "\n",
        "# Display or save the final table\n",
        "grouped"
      ],
      "metadata": {
        "id": "ikBM6PW098vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predictions_df.to_pickle('/content/drive/Shareddrives/DHKP/2021-2023.기존연구/03. AI융합연구_우리나라_저희(21년_9-12월)/[서재현] 저희/학습용데이터/v3_jeohui_040730_third_data_predictions.pickle')"
      ],
      "metadata": {
        "id": "_pGtjQgbdZcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Data based on the provided values for the table\n",
        "data = {\n",
        "    \"저편의 저희\": [\"488(47.10%)\", \"505(29.99%)\", \"499(25.92%)\", \"466(18.40%)\", \"334(10.28%)\", \"140(8.51%)\", \"82(0.91%)\", \"68(0.58%)\", \"2582(21%)\"],\n",
        "    \"겸양의 저희\": [\"548(52.90%)\", \"1179(70.01%)\", \"1426(74.08%)\", \"2067(81.60%)\", \"2916(89.72%)\", \"1506(91.49%)\", \"8976(99.09%)\", \"11603(99.42%)\", \"30221(79%)\"],\n",
        "    \"전체 저희\": [1036, 1684, 1925, 2533, 3250, 1646, 9058, 11671, 32803]\n",
        "}\n",
        "\n",
        "# Create the index for the years\n",
        "index = [\"50s\", \"60s\", \"70s\", \"80s\", \"90s\", \"2000s\", \"2010s\", \"2020s\", \"총합\"]\n",
        "\n",
        "# Create the dataframe\n",
        "df = pd.DataFrame(data, index=index)\n",
        "\n",
        "# Plot the table using Matplotlib\n",
        "fig, ax = plt.subplots(figsize=(10, 4))  # set the figure size\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "\n",
        "# Create table\n",
        "table = ax.table(cellText=df.values, colLabels=df.columns, rowLabels=df.index, loc='center', cellLoc='center')\n",
        "\n",
        "# Style the table (optional but helpful to match your format)\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(12)\n",
        "table.scale(1.2, 1.2)\n",
        "\n",
        "# Display the table\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jATHmlxOA28f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"겸양의 저희\"_높은 신뢰도 문장 필터링"
      ],
      "metadata": {
        "id": "ZmkO8_d506nI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 클래스 1에 대한 확률이 0.9 이상인 문장 필터링\n",
        "high_confidence_class_1 = predictions_df[predictions_df['prob_class_1'] >= 0.9]\n",
        "\n",
        "# 결과 확인\n",
        "print(\"High Confidence Predictions:\")\n",
        "print(high_confidence_class_1.head())\n"
      ],
      "metadata": {
        "id": "hRtW6T4QyyLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "high_confidence_class_1.to_excel('high_confidence_class_1.xlsx')"
      ],
      "metadata": {
        "id": "0bvXvxPUdssl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "high_confidence_class_1"
      ],
      "metadata": {
        "id": "ELa1qbTtXvDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"저편의 저희\" 신뢰도 높은 문장 필터링"
      ],
      "metadata": {
        "id": "fQRe0igeYT5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 클래스 0에 대한 확률이 0.9 이상인 문장 필터링\n",
        "#high_confidence_class_0 = predictions_df[predictions_df['prob_class_0'] >= predictions_df['prob_class_1']]\n",
        "high_confidence_class_0 = predictions_df[predictions_df['prob_class_0'] >= 0.8]\n",
        "\n",
        "# 결과 확인\n",
        "print(\"High Confidence Class 0 Predictions:\")\n",
        "print(high_confidence_class_0.head())\n"
      ],
      "metadata": {
        "id": "ODX5avXEYTdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "high_confidence_class_0.to_excel('high_confidence_class_0.xlsx')"
      ],
      "metadata": {
        "id": "yOwtWjcsdwvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "high_confidence_class_0"
      ],
      "metadata": {
        "id": "Oq7-JyGZZV71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "예측이 불확실한 문장 필터링\n"
      ],
      "metadata": {
        "id": "mb2j357D07Fm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 클래스 0과 1에 대한 확률 차이가 0.1 이하인 문장 필터링\n",
        "uncertain_predictions = predictions_df[abs(predictions_df['prob_class_0'] - predictions_df['prob_class_1']) <= 0.2]\n",
        "\n",
        "# 결과 확인\n",
        "print(\"Uncertain Predictions:\")\n",
        "print(uncertain_predictions.head())\n"
      ],
      "metadata": {
        "id": "rp6WM9o00VGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uncertain_predictions.to_excel('uncerain_predictions.xlsx')"
      ],
      "metadata": {
        "id": "lAjBVA0udDWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "클래스 1로 예측된 문장들 추출 및 통계 정보 확인\n"
      ],
      "metadata": {
        "id": "zIu594r4091s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 클래스 1로 예측된 문장들 추출\n",
        "class_1_predictions = predictions_df[predictions_df['predicted_label'] == 1]\n",
        "\n",
        "# 추가 작업 예시: 클래스 1로 예측된 문장들의 통계 정보 확인\n",
        "print(\"Class 1 Predictions Statistics:\")\n",
        "print(class_1_predictions.describe())\n"
      ],
      "metadata": {
        "id": "BH2seOLb0-I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_1_predictions.to_excel('class_1_predictions.xlsx')"
      ],
      "metadata": {
        "id": "kvkxRw3LeV1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 클래스 0으로 예측된 문장들 추출\n",
        "class_0_predictions = predictions_df[predictions_df['predicted_label'] == 0]\n",
        "\n",
        "# 추가 작업 예시: 클래스 0으로 예측된 문장들의 통계 정보 확인\n",
        "print(\"Class 0 Predictions Statistics:\")\n",
        "print(class_0_predictions.describe())\n"
      ],
      "metadata": {
        "id": "W-oatARbZyu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_0_predictions.to_excel('class_0_predictions.xlsx')"
      ],
      "metadata": {
        "id": "nZjzPq18eSel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "혼동 행렬 및 분류 보고서 생성\n"
      ],
      "metadata": {
        "id": "ZgCzxTN71BOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "실제 레이블이 있는 경우 혼동 행렬을 계산하여 모델의 성능을 평가할 수 있습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "hpLZNwIF1C-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "Hh65V4zYHA8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# 실제 레이블과 예측 레이블이 모두 있는 경우\n",
        "if 'actual_label' in test.columns:\n",
        "    y_true = predictions_df['actual_label']\n",
        "    y_pred = predictions_df['predicted_label']\n",
        "\n",
        "    # 혼동 행렬 계산\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "\n",
        "    # 분류 보고서 생성\n",
        "    class_report = classification_report(y_true, y_pred)\n",
        "    print(\"Classification Report:\")\n",
        "    print(class_report)\n",
        "else:\n",
        "    print(\"Actual labels are not available in the dataset.\")\n"
      ],
      "metadata": {
        "id": "t2-CiLTFG-zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# 실제 레이블과 예측 레이블이 모두 있는 경우\n",
        "if 'actual_label' in predictions_df.columns:\n",
        "    y_true = predictions_df['actual_label']\n",
        "    y_pred = predictions_df['predicted_label']\n",
        "\n",
        "    # 혼동 행렬 계산\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "\n",
        "    # 분류 보고서 생성\n",
        "    class_report = classification_report(y_true, y_pred)\n",
        "    print(\"Classification Report:\")\n",
        "    print(class_report)\n",
        "else:\n",
        "    print(\"Actual labels are not available in the dataset.\")\n"
      ],
      "metadata": {
        "id": "Hy5YFMV71DyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW2yL3uU0BNL"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from adjustText import adjust_text\n",
        "\n",
        "# 연도별 데이터 프레임 생성 함수\n",
        "def create_yearly_df(df):\n",
        "    yearly_df = df.groupby(['year', 'predicted_label']).size().unstack(fill_value=0)\n",
        "    yearly_df.columns = ['지칭의 저희', '겸양의 저희']\n",
        "    yearly_df.reset_index(inplace=True)\n",
        "    return yearly_df\n",
        "\n",
        "# '겸양의 저희'와 '지칭의 저희' 연도별 빈도 추이 그래프 그리기\n",
        "def plot_trend(yearly_df):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # 원 데이터 플롯\n",
        "    plt.plot(yearly_df['year'], yearly_df['겸양의 저희'], marker='o', linestyle='-', color='b', label='겸양의 저희')\n",
        "    plt.plot(yearly_df['year'], yearly_df['지칭의 저희'], marker='o', linestyle='-', color='r', label='지칭의 저희')\n",
        "\n",
        "    # '겸양의 저희' 추세선 추가\n",
        "    z1 = np.polyfit(yearly_df['year'], yearly_df['겸양의 저희'], 1)\n",
        "    p1 = np.poly1d(z1)\n",
        "    plt.plot(yearly_df['year'], p1(yearly_df['year']), linestyle='--', color='b', alpha=0.5)\n",
        "\n",
        "    # '지칭의 저희' 추세선 추가\n",
        "    z2 = np.polyfit(yearly_df['year'], yearly_df['지칭의 저희'], 1)\n",
        "    p2 = np.poly1d(z2)\n",
        "    plt.plot(yearly_df['year'], p2(yearly_df['year']), linestyle='--', color='r', alpha=0.5)\n",
        "\n",
        "    plt.title('신문기사 연도별 저희 추세 그래프')\n",
        "    plt.xlabel('연도')\n",
        "    plt.ylabel('빈도수')\n",
        "    plt.grid(True)\n",
        "\n",
        "    texts = []\n",
        "    for i, row in yearly_df.iterrows():\n",
        "        texts.append(plt.text(row['year'], row['겸양의 저희'], row['겸양의 저희'], ha='center', va='bottom', fontsize=6.5, color='black'))\n",
        "        texts.append(plt.text(row['year'], row['지칭의 저희'], row['지칭의 저희'], ha='center', va='top', fontsize=6.5, color='black'))\n",
        "\n",
        "    adjust_text(texts, arrowprops=dict(arrowstyle=\"->\", color='gray', lw=0.5))\n",
        "\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.xticks(yearly_df['year'], rotation=90)\n",
        "    plt.show()\n",
        "\n",
        "# 연도별 데이터 프레임 생성\n",
        "yearly_df = create_yearly_df(predictions_df)\n",
        "\n",
        "# 연도별 추이 그래프 그리기\n",
        "plot_trend(yearly_df)\n"
      ],
      "metadata": {
        "id": "nFb_kBluvRJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from adjustText import adjust_text\n",
        "\n",
        "# 연대별 데이터 프레임 생성 함수\n",
        "def create_decade_df(df):\n",
        "    df['decade'] = (df['year'] // 10) * 10\n",
        "    decade_df = df.groupby(['decade', 'predicted_label']).size().unstack(fill_value=0)\n",
        "    decade_df.columns = ['지칭의 저희', '겸양의 저희']\n",
        "    decade_df.reset_index(inplace=True)\n",
        "    return decade_df\n",
        "\n",
        "# '겸양의 저희'와 '지칭의 저희' 연대별 빈도 추이 그래프 그리기\n",
        "def plot_trend(decade_df):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # 원 데이터 플롯\n",
        "    plt.plot(decade_df['decade'], decade_df['겸양의 저희'], marker='o', linestyle='-', color='b', label='겸양의 저희')\n",
        "    plt.plot(decade_df['decade'], decade_df['지칭의 저희'], marker='o', linestyle='-', color='r', label='지칭의 저희')\n",
        "\n",
        "    # '겸양의 저희' 추세선 추가\n",
        "    z1 = np.polyfit(decade_df['decade'], decade_df['겸양의 저희'], 1)\n",
        "    p1 = np.poly1d(z1)\n",
        "    plt.plot(decade_df['decade'], p1(decade_df['decade']), linestyle='--', color='b', alpha=0.5)\n",
        "\n",
        "    # '지칭의 저희' 추세선 추가\n",
        "    z2 = np.polyfit(decade_df['decade'], decade_df['지칭의 저희'], 1)\n",
        "    p2 = np.poly1d(z2)\n",
        "    plt.plot(decade_df['decade'], p2(decade_df['decade']), linestyle='--', color='r', alpha=0.5)\n",
        "\n",
        "    plt.title('신문기사 연대별 저희 추세 그래프')\n",
        "    plt.xlabel('연대')\n",
        "    plt.ylabel('빈도수')\n",
        "    plt.grid(True)\n",
        "\n",
        "    texts = []\n",
        "    for i, row in decade_df.iterrows():\n",
        "        texts.append(plt.text(row['decade'], row['겸양의 저희'], row['겸양의 저희'], ha='center', va='bottom', fontsize=9, color='black'))\n",
        "        texts.append(plt.text(row['decade'], row['지칭의 저희'], row['지칭의 저희'], ha='center', va='top', fontsize=9, color='black'))\n",
        "\n",
        "    adjust_text(texts, arrowprops=dict(arrowstyle=\"->\", color='gray', lw=0.5))\n",
        "\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.xticks(decade_df['decade'])\n",
        "    plt.show()\n",
        "\n",
        "# 연대별 데이터 프레임 생성\n",
        "decade_df = create_decade_df(predictions_df)\n",
        "\n",
        "# 연대별 추이 그래프 그리기\n",
        "plot_trend(decade_df)\n"
      ],
      "metadata": {
        "id": "yOB4rO2QvuPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from adjustText import adjust_text\n",
        "\n",
        "# 연도별 데이터 프레임 생성 함수\n",
        "def create_yearly_df(df):\n",
        "    yearly_df = df.groupby(['year', 'predicted_label']).size().unstack(fill_value=0)\n",
        "    yearly_df.columns = ['지칭의 저희', '겸양의 저희']\n",
        "    yearly_df.reset_index(inplace=True)\n",
        "    yearly_df['차이'] = yearly_df['겸양의 저희'] - yearly_df['지칭의 저희']\n",
        "    return yearly_df\n",
        "\n",
        "# 시각화 함수\n",
        "def plot_difference(yearly_df):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # 원 데이터 플롯\n",
        "    plt.plot(yearly_df['year'], yearly_df['차이'], marker='o', linestyle='-', color='g', label='겸양의 저희 - 지칭의 저희')\n",
        "\n",
        "    # 차이 추세선 추가\n",
        "    z = np.polyfit(yearly_df['year'], yearly_df['차이'], 1)\n",
        "    p = np.poly1d(z)\n",
        "    plt.plot(yearly_df['year'], p(yearly_df['year']), linestyle='--', color='g', alpha=0.5)\n",
        "\n",
        "    plt.title('연도별 겸양의 저희와 지칭의 저희의 차이')\n",
        "    plt.xlabel('연도')\n",
        "    plt.ylabel('차이 (겸양의 저희 - 지칭의 저희)')\n",
        "    plt.grid(True)\n",
        "\n",
        "    texts = []\n",
        "    for i, row in yearly_df.iterrows():\n",
        "        texts.append(plt.text(row['year'], row['차이'], row['차이'], ha='center', va='bottom', fontsize=9, color='black'))\n",
        "\n",
        "    adjust_text(texts, arrowprops=dict(arrowstyle=\"->\", color='gray', lw=0.5))\n",
        "\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.xticks(yearly_df['year'], rotation=90)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "7TkuhfqI6glf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터프레임 확인\n",
        "print(\"데이터프레임 확인:\")\n",
        "print(predictions_df)"
      ],
      "metadata": {
        "id": "hUWUGgtH6wMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 연도별 데이터 프레임 생성\n",
        "yearly_df = create_yearly_df(predictions_df)\n",
        "\n",
        "# 연도별 데이터프레임 확인\n",
        "print(\"연도별 데이터프레임 확인:\")\n",
        "yearly_df"
      ],
      "metadata": {
        "id": "V_M7TSTT6yjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 연도별 겸양의 저희와 지칭의 저희의 차이 추이 그래프 그리기\n",
        "plot_difference(yearly_df)"
      ],
      "metadata": {
        "id": "YjGilKAQ60Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from adjustText import adjust_text\n",
        "\n",
        "# 연도별 데이터 프레임 생성 함수\n",
        "def create_yearly_df(df):\n",
        "    yearly_df = df.groupby(['year', 'predicted_label']).size().unstack(fill_value=0)\n",
        "    yearly_df.columns = ['지칭의 저희', '겸양의 저희']\n",
        "    yearly_df.reset_index(inplace=True)\n",
        "    yearly_df['차이'] = yearly_df['겸양의 저희'] - yearly_df['지칭의 저희']\n",
        "    return yearly_df\n",
        "\n",
        "# 시각화 함수\n",
        "def plot_difference(yearly_df):\n",
        "    plt.figure(figsize=(14, 8))\n",
        "\n",
        "    # 원 데이터 플롯\n",
        "    plt.plot(yearly_df['year'], yearly_df['차이'], marker='o', markersize=10, linestyle='-', linewidth=2, color='darkgreen', label='겸양의 저희 - 지칭의 저희')\n",
        "\n",
        "    # 차이 추세선 추가\n",
        "    z = np.polyfit(yearly_df['year'], yearly_df['차이'], 1)\n",
        "    p = np.poly1d(z)\n",
        "    plt.plot(yearly_df['year'], p(yearly_df['year']), linestyle='--', linewidth=2, color='orange', alpha=0.7, label='추세선')\n",
        "\n",
        "    plt.title('연도별 겸양의 저희와 지칭의 저희의 차이', fontsize=16)\n",
        "    plt.xlabel('연도', fontsize=14)\n",
        "    plt.ylabel('차이 (겸양의 저희 - 지칭의 저희)', fontsize=14)\n",
        "    plt.grid(True)\n",
        "\n",
        "    # 텍스트 레이블 추가\n",
        "    texts = []\n",
        "    for i, row in yearly_df.iterrows():\n",
        "        texts.append(plt.text(row['year'], row['차이'], f'{row[\"차이\"]}', ha='center', va='bottom', fontsize=12, color='black'))\n",
        "\n",
        "    adjust_text(texts, arrowprops=dict(arrowstyle=\"->\", color='gray', lw=0.5))\n",
        "\n",
        "    plt.legend(loc='upper left', fontsize=12)\n",
        "    plt.xticks(yearly_df['year'], rotation=45, fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "    plt.ylim(min(yearly_df['차이']) - 2, max(yearly_df['차이']) + 2)  # y축 범위 조정\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 데이터프레임 확인\n",
        "print(\"데이터프레임 확인:\")\n",
        "print(predictions_df)\n",
        "\n",
        "# 연도별 데이터 프레임 생성\n",
        "yearly_df = create_yearly_df(predictions_df)\n",
        "\n",
        "# 연도별 데이터프레임 확인\n",
        "print(\"연도별 데이터프레임 확인:\")\n",
        "print(yearly_df)\n",
        "\n",
        "# 연도별 겸양의 저희와 지칭의 저희의 차이 추이 그래프 그리기\n",
        "plot_difference(yearly_df)\n"
      ],
      "metadata": {
        "id": "hk9u3T3h7HBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_df = pd.read_pickle('/content/drive/Shareddrives/DHKP/2021-2023.기존연구/03. AI융합연구_우리나라_저희(21년_9-12월)/[서재현] 저희/학습용데이터/v3_jeohui_040730_third_data_predictions.pickle')"
      ],
      "metadata": {
        "id": "INQu8gSg34zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_df.columns"
      ],
      "metadata": {
        "id": "hPl-BcGW3Twm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = predictions_df"
      ],
      "metadata": {
        "id": "RPlF2IKg37ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from adjustText import adjust_text\n",
        "\n",
        "# 연도별 데이터 프레임 생성 함수\n",
        "def create_yearly_df(df):\n",
        "    yearly_df = df.groupby(['year', 'predicted_label']).size().unstack(fill_value=0)\n",
        "    yearly_df.columns = ['지칭의 저희', '겸양의 저희']\n",
        "    yearly_df.reset_index(inplace=True)\n",
        "    yearly_df['차이'] = yearly_df['겸양의 저희'] - yearly_df['지칭의 저희']\n",
        "    return yearly_df\n",
        "\n",
        "# 시각화 함수\n",
        "def plot_difference(yearly_df):\n",
        "    plt.figure(figsize=(14, 8))\n",
        "\n",
        "    # 막대그래프\n",
        "    bars = plt.bar(yearly_df['year'], yearly_df['차이'], color=np.where(yearly_df['차이'] >= 0, 'blue', 'red'))\n",
        "\n",
        "    plt.title('연도별 겸양의 저희와 지칭의 저희의 차이', fontsize=16)\n",
        "    plt.xlabel('연도', fontsize=14)\n",
        "    plt.ylabel('차이 (겸양의 저희 - 지칭의 저희)', fontsize=14)\n",
        "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # 데이터 라벨 추가\n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, yval, int(yval), ha='center', va='bottom' if yval >= 0 else 'top', fontsize=12)\n",
        "\n",
        "    plt.xticks(yearly_df['year'], rotation=45, fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 연도별 겸양의 저희와 지칭의 저희의 차이 추이 그래프 그리기\n",
        "plot_difference(yearly_df)"
      ],
      "metadata": {
        "id": "PP7SEPrz2fOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from adjustText import adjust_text\n",
        "\n",
        "# 연대별 데이터 프레임 생성 함수\n",
        "def create_decade_df(df):\n",
        "    df['decade'] = (df['year'] // 10) * 10\n",
        "    decade_df = df.groupby(['decade', 'predicted_label']).size().unstack(fill_value=0)\n",
        "    decade_df.columns = ['지칭의 저희', '겸양의 저희']\n",
        "    decade_df['전체 기사 수'] = decade_df.sum(axis=1)\n",
        "    decade_df['지칭의 저희 비율'] = decade_df['지칭의 저희'] / decade_df['전체 기사 수']\n",
        "    decade_df['겸양의 저희 비율'] = decade_df['겸양의 저희'] / decade_df['전체 기사 수']\n",
        "    decade_df.reset_index(inplace=True)\n",
        "    return decade_df\n",
        "\n",
        "# 시각화 함수\n",
        "def plot_decade_proportions(decade_df):\n",
        "    plt.figure(figsize=(14, 8))\n",
        "\n",
        "    # 원 데이터 플롯\n",
        "    plt.plot(decade_df['decade'], decade_df['겸양의 저희 비율'], marker='o', linestyle='-', linewidth=2, color='blue', label='겸양의 저희 비율')\n",
        "    plt.plot(decade_df['decade'], decade_df['지칭의 저희 비율'], marker='o', linestyle='-', linewidth=2, color='red', label='지칭의 저희 비율')\n",
        "\n",
        "    # 겸양의 저희 비율 추세선 추가\n",
        "    z1 = np.polyfit(decade_df['decade'], decade_df['겸양의 저희 비율'], 1)\n",
        "    p1 = np.poly1d(z1)\n",
        "    plt.plot(decade_df['decade'], p1(decade_df['decade']), linestyle='--', linewidth=2, color='blue', alpha=0.5)\n",
        "\n",
        "    # 지칭의 저희 비율 추세선 추가\n",
        "    z2 = np.polyfit(decade_df['decade'], decade_df['지칭의 저희 비율'], 1)\n",
        "    p2 = np.poly1d(z2)\n",
        "    plt.plot(decade_df['decade'], p2(decade_df['decade']), linestyle='--', linewidth=2, color='red', alpha=0.5)\n",
        "\n",
        "    plt.title('연대별 전체 기사 수 대비 겸양의 저희와 지칭의 저희 비율', fontsize=16)\n",
        "    plt.xlabel('연대', fontsize=14)\n",
        "    plt.ylabel('비율', fontsize=14)\n",
        "    plt.grid(True)\n",
        "\n",
        "    texts = []\n",
        "    for i, row in decade_df.iterrows():\n",
        "        texts.append(plt.text(row['decade'], row['겸양의 저희 비율'], f'{row[\"겸양의 저희 비율\"]:.2%}', ha='center', va='bottom', fontsize=12, color='blue'))\n",
        "        texts.append(plt.text(row['decade'], row['지칭의 저희 비율'], f'{row[\"지칭의 저희 비율\"]:.2%}', ha='center', va='top', fontsize=12, color='red'))\n",
        "\n",
        "    adjust_text(texts, arrowprops=dict(arrowstyle=\"->\", color='gray', lw=0.5))\n",
        "\n",
        "    plt.legend(loc='upper left', fontsize=12)\n",
        "    plt.xticks(decade_df['decade'], rotation=45, fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 데이터프레임 확인\n",
        "print(\"데이터프레임 확인:\")\n",
        "print(predictions_df)\n",
        "\n",
        "# 연대별 데이터 프레임 생성\n",
        "decade_df = create_decade_df(predictions_df)\n",
        "\n",
        "# 연대별 데이터프레임 확인\n",
        "print(\"연대별 데이터프레임 확인:\")\n",
        "print(decade_df)\n",
        "\n",
        "# 연대별 전체 기사 수 대비 겸양의 저희와 지칭의 저희 비율 추이 그래프 그리기\n",
        "plot_decade_proportions(decade_df)\n"
      ],
      "metadata": {
        "id": "8nEEmPxP8Azb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 주어진 데이터를 딕셔너리로 변환\n",
        "data = {\n",
        "    'year': [1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999,\n",
        "             2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009,\n",
        "             2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019,\n",
        "             2020, 2021, 2022, 2023, 2024],\n",
        "    'count': [5820, 5776, 5117, 5163, 5173, 4931, 4912, 5735, 4790, 4659,\n",
        "              10179, 10582, 9170, 7976, 7702, 6981, 7071, 7651, 7643, 8437,\n",
        "              8200, 7925, 7619, 7147, 6782, 7071, 6960, 7280, 23759, 28942,\n",
        "              32976, 24956, 26334, 24675, 14219]\n",
        "}\n",
        "\n",
        "# 데이터프레임으로 변환\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 시각화\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.plot(df['year'], df['count'], marker='o', linestyle='-', linewidth=2, color='purple', label='우리의 사용 추이')\n",
        "\n",
        "# 추세선 추가\n",
        "z = np.polyfit(df['year'], df['count'], 1)\n",
        "p = np.poly1d(z)\n",
        "plt.plot(df['year'], p(df['year']), linestyle='--', linewidth=2, color='orange', alpha=0.5, label='추세선')\n",
        "\n",
        "# 그래프 제목과 축 레이블 추가\n",
        "plt.title(\"'우리'의 사용 추이\", fontsize=16)\n",
        "plt.xlabel('연도', fontsize=14)\n",
        "plt.ylabel('사용 빈도', fontsize=14)\n",
        "plt.grid(True)\n",
        "\n",
        "# 데이터 포인트 레이블 추가\n",
        "for i, row in df.iterrows():\n",
        "    plt.text(row['year'], row['count'], f'{row[\"count\"]}', ha='center', va='bottom' if row[\"count\"] > 0 else 'top', fontsize=10)\n",
        "\n",
        "# 범례 추가\n",
        "plt.legend(loc='upper left', fontsize=12)\n",
        "plt.xticks(df['year'], rotation=45, fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AY-fM13T_YrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 연대별 데이터프레임 확인\n",
        "print(\"연대별 데이터프레임 확인:\")\n",
        "print(decade_df)"
      ],
      "metadata": {
        "id": "fU8wyTHi5Aqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 동아일보 연대별 추이 그래프 그리기\n",
        "plot_trend(decade_df, 'donga')"
      ],
      "metadata": {
        "id": "lPxo6KNv2uWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 조선일보 연대별 추이 그래프 그리기\n",
        "plot_trend(decade_df, 'chosun')"
      ],
      "metadata": {
        "id": "Saslad4W4EFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVlMi4cd0BNP"
      },
      "source": [
        "#### 전체"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfRr4MS10BNP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "plt.rc('font', family='NanumBarunGothic')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIQ3XANp0BNP"
      },
      "outputs": [],
      "source": [
        "df=predictions_df.groupby(['year', 'predicted_label']).count()\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyt_QsUt0BNP"
      },
      "outputs": [],
      "source": [
        "df.reset_index(inplace=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc2De6M70BNP"
      },
      "outputs": [],
      "source": [
        "df.set_index(['year'], inplace=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-wbXCzM0BNP"
      },
      "outputs": [],
      "source": [
        "df['겸양의 저희'] =0\n",
        "df['지칭의 저희'] = 0\n",
        "\n",
        "for idx in df.index :\n",
        "    df['겸양의 저희'][idx] = df['text'][df['predicted_label']==1][idx]\n",
        "\n",
        "    try :\n",
        "        df['지칭의 저희'][idx] =df['text'][df['predicted_label']==0][idx]\n",
        "    except :\n",
        "    #donga['겸양의 저희'][idx] =0\n",
        "        df['지칭의 저희'][idx] =0\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRB9QUC50BNP"
      },
      "outputs": [],
      "source": [
        "df= df[['겸양의 저희', '지칭의 저희']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BINxzqZ0BNP"
      },
      "outputs": [],
      "source": [
        "df.reset_index(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNTbk3Z90BNP"
      },
      "outputs": [],
      "source": [
        "df= df.drop_duplicates('year')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gldnd0TY0BNQ"
      },
      "outputs": [],
      "source": [
        "df.set_index('year', inplace=True)\n",
        "df.plot()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = predictions_df\n",
        "df"
      ],
      "metadata": {
        "id": "2DF-KzB3ozq2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XCgV7Qdl0BNQ",
        "SJFy3cPlj9I5",
        "pkErXIAD0BNS",
        "qWCxC4RW0BNU",
        "vJiAJPUDz40W",
        "wmou0LFl0R_X"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "e815ec34c9d7ca6d5f6dd6120341f05865440853c6373931621ba05a6d8d6b08"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}